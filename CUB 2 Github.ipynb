{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85f70998",
   "metadata": {},
   "source": [
    "+ 8855 imágenes de train \n",
    "+ 2933 imágenes de test\n",
    "+ 10 captions por imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b783d7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.random as random\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict # diccionario que al asignar un nuevo valor crea la clave\n",
    "if sys.version_info[0] == 2:\n",
    "    import cPickle as pickle\n",
    "else:\n",
    "    import pickle\n",
    "    \n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer # A tokenizer that splits a string using a regular expression, which matches either the tokens or the separators between tokens.\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "\n",
    "from transformers import CLIPTokenizer, TFCLIPTextModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9915c7",
   "metadata": {},
   "source": [
    "# PARAMS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5179fee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/birds\"\n",
    "EMBEDDINGS_NUM = 10 # OLD número de captions por imagen\n",
    "WORDS_NUM = 18 # número maximo de palabras por caption. Si es menos se hace padding, si es mas se trunca\n",
    "BRANCH_NUM = 0  # número de veces que cargo cada imagen. Si es 0 cargamos solo la imagen del tamaño dado\n",
    "BASE_SIZE = 64 # Tamaño mínimo de las imagenes. Cada imagen que cargaremos tendrá el doble de tamaño que la anterior. la última el tamaño original \n",
    "NOISE_SIZE = 128 # power of 2\n",
    "CONDITION_DIM = 100\n",
    "OUT_EMB = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6582a7",
   "metadata": {},
   "source": [
    "Guía de funcioens de carga\n",
    "https://github.com/mrlibw/ControlGAN/blob/master/code/datasets.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe414e05",
   "metadata": {},
   "source": [
    "# LOAD AND PREPARE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9e8018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PREPARE_CAPTIONS():\n",
    "    \"\"\"\n",
    "    CLASE PARA PREPARAR EL PICKLE DE CAPTIONS. \n",
    "    PASAMOS A INDÍCES LOS CAPTIONS (SEGÚN VOCABULARIO CREADO)\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, split='train'):\n",
    "        self.EMBEDDINGS_NUM = EMBEDDINGS_NUM\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "    def load_filenames(self, data_dir, filenames):\n",
    "        \"\"\"\n",
    "        Cargamos los pickles de los filenames. Contienen los names de la imagen/caption.\n",
    "        \"\"\"\n",
    "\n",
    "        filepath = '%s/%s/filenames.pickle' % (data_dir, split)\n",
    "        if os.path.isfile(filepath):\n",
    "            with open(filepath, 'rb') as f:\n",
    "                filenames = pickle.load(f)\n",
    "            print('Load filenames from: %s (%d)' % (filepath, len(filenames)))\n",
    "        else:\n",
    "            filenames = []\n",
    "        return filenames\n",
    "\n",
    "    def load_captions(self, data_dir, filenames):\n",
    "        all_captions = []\n",
    "        for i in range(len(filenames)):\n",
    "            cap_path = '%s/text/%s.txt' % (data_dir, filenames[i])\n",
    "            with open(cap_path, \"r\") as f:\n",
    "                # Leemos cada línea y lo metemos en una lista \n",
    "                captions = f.read().split('\\n')\n",
    "                cnt = 0\n",
    "                for cap in captions:\n",
    "                    if len(cap) == 0:\n",
    "                        continue\n",
    "                    cap = cap.replace(\"\\ufffd\\ufffd\", \" \")\n",
    "                    # Nos quedamos solo con secuencias alfanuméricas como tokens, borramos el resto\n",
    "#                    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#                    tokens = tokenizer.tokenize(cap.lower())\n",
    "#                    if len(tokens) == 0:\n",
    "#                        print('cap', cap)\n",
    "#                        continue\n",
    "#                    tokens_new = []\n",
    "#                    for t in tokens:\n",
    "#                        t = t.encode('ascii', 'ignore').decode('ascii')\n",
    "#                        if len(t) > 0:\n",
    "#                            tokens_new.append(t)\n",
    "#                    all_captions.append(tokens_new)\n",
    "                    all_captions.append(cap)\n",
    "#                    cnt += 1\n",
    "#                    if cnt == self.EMBEDDINGS_NUM:\n",
    "#                        break\n",
    "#                if cnt < self.EMBEDDINGS_NUM:\n",
    "#                    print('ERROR: the captions for %s less than %d' % (filenames[i], cnt))\n",
    "        return all_captions\n",
    "\n",
    "    def build_dictionary(self, train_captions, test_captions):\n",
    "        \"\"\"\n",
    "        Creo diccionarios. \n",
    "        Vocabulario.\n",
    "        Captions como indices.\n",
    "        Indíces a words\n",
    "        Words a índices\n",
    "        Número de palabras.\n",
    "        \"\"\"\n",
    "\n",
    "        # Creo diccionario de palabras existentes: número de aparaciones \n",
    "        word_counts = defaultdict(float)\n",
    "        captions = train_captions + test_captions\n",
    "        for sent in captions:\n",
    "            for word in sent:\n",
    "                word_counts[word] += 1\n",
    "\n",
    "        # Creo vocabulario\n",
    "        vocab = [w for w in word_counts if word_counts[w] >= 0]\n",
    "        # Añado al vocabulario \"end\"\n",
    "        # Creo los intercambios de indices-palabras y viceversa\n",
    "        ixtoword = {}\n",
    "        ixtoword[0] = '<end>'\n",
    "        wordtoix = {}\n",
    "        wordtoix['<end>'] = 0\n",
    "        ix = 1\n",
    "        for w in vocab:\n",
    "            wordtoix[w] = ix\n",
    "            ixtoword[ix] = w\n",
    "            ix += 1\n",
    "\n",
    "        # Train: pasar los captions a indices\n",
    "        train_captions_new = []\n",
    "        for t in train_captions:\n",
    "            rev = []\n",
    "            for w in t:\n",
    "                if w in wordtoix:\n",
    "                    rev.append(wordtoix[w])\n",
    "            train_captions_new.append(rev)\n",
    "\n",
    "        # Test: pasar los captions a indices\n",
    "        test_captions_new = []\n",
    "        for t in test_captions:\n",
    "            rev = []\n",
    "            for w in t:\n",
    "                if w in wordtoix:\n",
    "                    rev.append(wordtoix[w])\n",
    "            test_captions_new.append(rev)\n",
    "\n",
    "        return [train_captions_new, test_captions_new, ixtoword, wordtoix, len(ixtoword)]\n",
    "\n",
    "    def load_text_data(self, data_dir, split):\n",
    "        \"\"\"\n",
    "        Funcion para leer los pickles segun splits\n",
    "        Return: \n",
    "            + filenames: \n",
    "            + captions: \n",
    "            + ixtoword: \n",
    "            + wordtoix: \n",
    "            + n_words:  número de palabras\n",
    "        \"\"\"\n",
    "        # Cargamos los pickles de los splits:\n",
    "        filepath = os.path.join(data_dir, 'captions.pickle')\n",
    "        train_names = self.load_filenames(data_dir, 'train')\n",
    "        test_names = self.load_filenames(data_dir, 'test')\n",
    "\n",
    "        # Si no existe creamos el captions.pickle con los captions como indices y los diccionarios \n",
    "        # para pasar al vocabulario\n",
    "        if not os.path.isfile(filepath):\n",
    "            # Cargamos los descriptores\n",
    "            train_captions = self.load_captions(data_dir, train_names)\n",
    "            test_captions = self.load_captions(data_dir, test_names)\n",
    "            # Diccionario de vocabulario (y de índice a palabras y viceversa)\n",
    "            # y paso los captions a índices:\n",
    "            #train_captions, test_captions, ixtoword, wordtoix, n_words = self.build_dictionary(train_captions, test_captions)\n",
    "            #Guardo todo esto en pickles\n",
    "            with open(filepath, 'wb') as f:\n",
    "                pickle.dump([train_captions, test_captions], f, protocol=2)#  ,ixtoword, wordtoix], f, protocol=2)\n",
    "                print('Save to: ', filepath)\n",
    "        # Si existe ccaptions.picke lo cargamos \n",
    "        else:\n",
    "            with open(filepath, 'rb') as f:\n",
    "                print(\"filepath\", filepath)\n",
    "                x = pickle.load(f)\n",
    "                train_captions, test_captions = x[0], x[1]\n",
    "                #ixtoword, wordtoix = x[2], x[3]\n",
    "                del x\n",
    "                #n_words = len(ixtoword)\n",
    "                print('Load from: ', filepath)\n",
    "        if split == 'train':\n",
    "            # a list of list: each list contains\n",
    "            # the indices of words in a sentence\n",
    "            captions = train_captions\n",
    "            filenames = train_names\n",
    "        else:  # split=='test'\n",
    "            captions = test_captions\n",
    "            filenames = test_names\n",
    "        return filenames, captions#, ixtoword, wordtoix, n_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c655f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PREPARE_IMAGE():\n",
    "    \n",
    "    def __init__(self, data_dir, split, captions, filenames, BASE_SIZE):\n",
    "        self.data_dir = data_dir\n",
    "        self.imsize = []\n",
    "        self.BASE_SIZE = BASE_SIZE\n",
    "        self.captions = captions\n",
    "        self.filenames = filenames \n",
    "        # tamaños a los que escalo\n",
    "        if BRANCH_NUM>0:\n",
    "            for i in range(BRANCH_NUM):\n",
    "                self.imsize.append(self.BASE_SIZE)\n",
    "                self.BASE_SIZE = self.BASE_SIZE * 2\n",
    "        else:\n",
    "            self.imsize.append(self.BASE_SIZE)\n",
    "        self.bbox = self.load_bbox()        \n",
    "        self.class_id = self.load_class_id(data_dir, split, len(self.filenames))\n",
    "    def get_imgs(self, img_path, imsize, bbox=None):\n",
    "        \"\"\"\n",
    "        Función de carga de imagen.\n",
    "        Realizo crop de la bbox si existe\n",
    "        \"\"\"\n",
    "        \n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        width, height = img.size\n",
    "        # Realizo crop de la bbox si existe\n",
    "        if bbox is not None:\n",
    "            r = int(np.maximum(bbox[2], bbox[3]) * 0.75)\n",
    "            center_x = int((2 * bbox[0] + bbox[2]) / 2)\n",
    "            center_y = int((2 * bbox[1] + bbox[3]) / 2)\n",
    "            y1 = np.maximum(0, center_y - r)\n",
    "            y2 = np.minimum(height, center_y + r)\n",
    "            x1 = np.maximum(0, center_x - r)\n",
    "            x2 = np.minimum(width, center_x + r)\n",
    "            img = img.crop([x1, y1, x2, y2])\n",
    "        #aumentado con flip\n",
    "        if np.random.rand()<0.5:\n",
    "            img = np.fliplr(img)\n",
    "        # Escalo la imagen a distintos tamaños \n",
    "        # La última de las veces NO\n",
    "        if BRANCH_NUM>0:\n",
    "            ret = []\n",
    "            for i in range(BRANCH_NUM):\n",
    "                if i < (BRANCH_NUM - 1):\n",
    "                    # re_img = transforms.Scale(imsize[i])(img)\n",
    "                    re_img = tf.image.resize(img, size=(imsize[i], imsize[i]))\n",
    "                else:\n",
    "                    re_img = img\n",
    "                ret.append((tf.cast(re_img, tf.float32) - 127.5) / 127.5)\n",
    "        else:\n",
    "            ret = tf.image.resize(img, size=(imsize[0], imsize[0]))\n",
    "            ret = (tf.cast(ret, tf.float32) - 127.5) / 127.5 #/255\n",
    "        return ret  \n",
    "    \n",
    "    def load_class_id(self, data_dir, split, total_num):\n",
    "        \"\"\"\n",
    "        Cargamos la información de a que clase pertenece (por id)\n",
    "        \"\"\"\n",
    "        split_dir = os.path.join(data_dir, split)\n",
    "        if os.path.isfile(split_dir + '/class_info.pickle'):\n",
    "            with open(split_dir + '/class_info.pickle', 'rb') as f:\n",
    "                class_id = pickle.load(f, encoding='latin1')\n",
    "        else:\n",
    "            class_id = np.arange(total_num)\n",
    "        return tf.convert_to_tensor(class_id)\n",
    "    \n",
    "    \n",
    "    def load_bbox(self):\n",
    "        \"\"\"\n",
    "        Leemos las BBox\n",
    "        \"\"\"\n",
    "        data_dir = self.data_dir\n",
    "        bbox_path = os.path.join(data_dir, 'bounding_boxes.txt')\n",
    "        df_bounding_boxes = pd.read_csv(bbox_path,\n",
    "                                        delim_whitespace=True,\n",
    "                                        header=None).astype(int)\n",
    "\n",
    "        filepath = os.path.join(data_dir, 'images.txt')\n",
    "        df_filenames = pd.read_csv(filepath, delim_whitespace=True, header=None)\n",
    "        filenames = df_filenames[1].tolist() #paths de todas las imágenes\n",
    "        print('Total filenames: ', len(filenames), filenames[0])\n",
    "        \n",
    "        # Paths BBOX\n",
    "        filename_bbox = {img_file[:-4]: [] for img_file in filenames}\n",
    "        numImgs = len(filenames)\n",
    "        \n",
    "        # Guardo en un diccionario el path y la bbox\n",
    "        for i in range(0, numImgs):\n",
    "            bbox = df_bounding_boxes.iloc[i][1:].tolist() #la bbox i\n",
    "            key = filenames[i][:-4]\n",
    "            filename_bbox[key] = bbox\n",
    "        return filename_bbox\n",
    "    \n",
    "    \n",
    "    def get_caption(self, sent_ix):\n",
    "        \"\"\"\n",
    "        sent_ix: numero random del 0 al embeddings num (número de frases por imagen)\n",
    "        Devuelve un vector de índices del número de palabras fijados (rellenado con 0 si es necesario) \n",
    "        \"\"\"\n",
    "        \n",
    "        # Selecciono una frase al azar\n",
    "        sent_caption = np.asarray(self.captions[sent_ix]).astype('int64')\n",
    "        if (sent_caption == 0).sum() > 0:\n",
    "            print('ERROR: do not need END (0) token', sent_caption)\n",
    "            \n",
    "        # Creo una vector del tamaño fijado de las frases. Relleno con 0 si hace falta\n",
    "        # si hay más palabras del número fijado selecciono al azar de la frase ese número de palabras\n",
    "        num_words = len(sent_caption)\n",
    "        x = np.zeros((WORDS_NUM, 1), dtype='int64')\n",
    "        x_len = num_words\n",
    "        if num_words <= WORDS_NUM:\n",
    "            x[:num_words, 0] = sent_caption\n",
    "        else:\n",
    "            ix = list(np.arange(num_words))  # 1, 2, 3,..., maxNum\n",
    "            np.random.shuffle(ix)\n",
    "            ix = ix[:WORDS_NUM]\n",
    "            ix = np.sort(ix)\n",
    "            x[:, 0] = sent_caption[ix]\n",
    "            x_len = WORDS_NUM\n",
    "        x = np.squeeze(x)\n",
    "\n",
    "        return tf.convert_to_tensor(x), tf.convert_to_tensor(x_len)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Dado un indice obtenemos:\n",
    "        + Imagen BRANCH_NUM veces. A distintos tamaños y recortada\n",
    "        + Un caption bueno (junto a su clase (tipo de pájaro))\n",
    "        + Un caption malo (junto a su clase (tipo de pájaro))\n",
    "        + Longitud del caption (fijada por WORDS_NUM)\n",
    "        \"\"\"\n",
    "        key = self.filenames[index]\n",
    "        cls_id = self.class_id[index]\n",
    "\n",
    "        if self.bbox is not None:\n",
    "            bbox = self.bbox[key]\n",
    "        #    data_dir = '%s/CUB_200_2011' % self.data_dir\n",
    "        else:\n",
    "            bbox = None\n",
    "        #    data_dir = self.data_dir\n",
    "        # Cargo la imagen BRANCH_NUM veces. A distintos tamaños y recortada\n",
    "        img_name = '%s/images/%s.jpg' % (data_dir, key)\n",
    "        imgs = self.get_imgs(img_name, self.imsize, bbox)\n",
    "        # Selecciono un caption al azar y lo cargo\n",
    "        sent_ix = random.randint(0, EMBEDDINGS_NUM)\n",
    "        new_sent_ix = index * EMBEDDINGS_NUM + sent_ix\n",
    "        #caps, cap_len = self.get_caption(new_sent_ix)\n",
    "        caps = self.captions[new_sent_ix]\n",
    "        \n",
    "        # Selecciono al azar un caption ERRONEO\n",
    "        wrong_idx = random.randint(0, len(self.filenames))\n",
    "        wrong_new_sent_ix = wrong_idx * EMBEDDINGS_NUM + sent_ix\n",
    "        #wrong_caps, wrong_cap_len = self.get_caption(wrong_new_sent_ix)\n",
    "        wrong_caps = self.captions[wrong_new_sent_ix]\n",
    "        wrong_cls_id = self.class_id[wrong_idx]\n",
    "\n",
    "        #return imgs, caps, cap_len, cls_id, key, wrong_caps, wrong_cap_len, wrong_cls_id\n",
    "        return imgs, caps, cls_id, key, wrong_caps, wrong_cls_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70f85ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load filenames from: ../data/birds/train/filenames.pickle (8855)\n",
      "Load filenames from: ../data/birds/train/filenames.pickle (8855)\n",
      "filepath ../data/birds\\captions.pickle\n",
      "Load from:  ../data/birds\\captions.pickle\n"
     ]
    }
   ],
   "source": [
    "split = \"train\"\n",
    "prepare_caption = PREPARE_CAPTIONS(data_dir, split)\n",
    "#filenames, captions, ixtoword, wordtoix, n_words = prepare_caption.load_text_data(data_dir, split)\n",
    "filenames, captions = prepare_caption.load_text_data(data_dir, split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05925451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total filenames:  11788 001.Black_footed_Albatross/Black_Footed_Albatross_0046_18.jpg\n"
     ]
    }
   ],
   "source": [
    "prepare_image = PREPARE_IMAGE(data_dir, split, captions, filenames, BASE_SIZE)\n",
    "#imgs, caps, cap_len, cls_id, key, wrong_caps, wrong_cap_len, wrong_cls_id = prepare_image.__getitem__(5)\n",
    "imgs, caps, cls_id, key, wrong_caps, wrong_cls_id = prepare_image.__getitem__(5)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "80af4a3e",
   "metadata": {},
   "source": [
    "plt.imshow(imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473786ac",
   "metadata": {},
   "source": [
    "# MODELOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98514737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisar posibilidad de no meter capas densas y simplemente hacer la lineal de la primera mitad de los elementos y sumar la sigmoid de la segunda mitad de los mismos\n",
    "class GatedLinearUnit(layers.Layer):\n",
    "    def __init__(self, units=200):\n",
    "        super(GatedLinearUnit, self).__init__()\n",
    "        #self.linear = layers.Dense(units)\n",
    "        #self.sigmoid = layers.Dense(units, activation=\"sigmoid\")\n",
    "    def call(self, inputs):\n",
    "        nc = inputs.shape[1]\n",
    "        assert nc % 2 == 0, 'channels dont divide 2!'\n",
    "        nc = int(nc/2)      \n",
    "        #return self.linear(inputs) * self.sigmoid(inputs)\n",
    "        return tf.math.multiply(inputs[:,:nc], tf.keras.activations.sigmoid(inputs[:, nc:]))\n",
    "\n",
    "class GatedLinearUnit2D(layers.Layer):\n",
    "    def __init__(self, units=200):\n",
    "        super(GatedLinearUnit2D, self).__init__()\n",
    "        #self.linear = layers.Dense(units)\n",
    "        #self.sigmoid = layers.Dense(units, activation=\"sigmoid\")\n",
    "    def call(self, inputs):\n",
    "        nc = inputs.shape[-1]\n",
    "        assert nc % 2 == 0, 'channels dont divide 2!'\n",
    "        nc = int(nc/2)      \n",
    "        #return self.linear(inputs) * self.sigmoid(inputs)\n",
    "        return tf.math.multiply(inputs[:,:,:,:nc], tf.keras.activations.sigmoid(inputs[:,:,:,nc:]))    \n",
    "\n",
    "def cond_aug():\n",
    "    inputs_emb = tf.keras.layers.Input(shape=(OUT_EMB,))\n",
    "    x = layers.Dense(CONDITION_DIM*4, use_bias=True)(inputs_emb)\n",
    "    x = GatedLinearUnit()(x)\n",
    "    mu = x[:, :CONDITION_DIM]\n",
    "    logvar = x[:, CONDITION_DIM:]\n",
    "    std = tf.math.exp(tf.math.multiply(logvar,0.5))\n",
    "    c_code = tf.random.normal([std.shape[1]], mean=0.0, stddev=1.0, dtype=tf.dtypes.float32)\n",
    "    c_code = tf.math.add(tf.math.multiply(std, c_code), mu)\n",
    "    return tf.keras.Model(inputs_emb, [c_code, mu, logvar], name=\"cond_aug\")\n",
    "\n",
    "projection_dims = 128\n",
    "def upsample(filters, size, apply_dropout=False,last = False):\n",
    "    initializer = tf.random_normal_initializer(0., 0.05)\n",
    "    result = tf.keras.Sequential()\n",
    "    if last:\n",
    "        result.add(tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
    "                                        padding='same',\n",
    "                                        kernel_initializer=initializer,\n",
    "                                        use_bias=False, activation='sigmoid'))\n",
    "    else:\n",
    "        result.add(\n",
    "        tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
    "                                        padding='same',\n",
    "                                        kernel_initializer=initializer,\n",
    "                                        use_bias=False))\n",
    "        result.add(tf.keras.layers.BatchNormalization())\n",
    "        if apply_dropout:\n",
    "            result.add(tf.keras.layers.Dropout(0.1))\n",
    "        result.add(tf.keras.layers.ReLU())\n",
    "    return result\n",
    "\n",
    "def Generator():\n",
    "    tam_ini = 256\n",
    "    ca_net = cond_aug()\n",
    "    \n",
    "    initializer = tf.random_normal_initializer(0., 0.05)\n",
    "    inputs_noise = tf.keras.layers.Input(shape=(NOISE_SIZE,))#output 100\n",
    "    inputs_emb = tf.keras.layers.Input(shape=(OUT_EMB,))\n",
    "    emb, mu, logvar = ca_net(inputs_emb)\n",
    "\n",
    "    #if projection_dims!=0:\n",
    "    #    emb = layers.Dense(projection_dims, use_bias=False)(inputs_emb) #output 128\n",
    "   # else:\n",
    "    #    emb = inputs_emb\n",
    "    x = tf.concat([inputs_noise, emb], 1)#output 228\n",
    "    # x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(4*4*tam_ini, use_bias=False)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = layers.Reshape((4,4, tam_ini))(x)\n",
    "    x = upsample(tam_ini, 3, apply_dropout=False)(x)\n",
    "    x = upsample(tam_ini//2, 3, apply_dropout=False)(x)\n",
    "    x = upsample(tam_ini//4, 3, apply_dropout=False)(x)\n",
    "    x = upsample(tam_ini//8, 3)(x)  \n",
    "    #x = upsample(tam_ini//16, 4)(x) \n",
    "    x = upsample(3, 3,last = True)(x)\n",
    "    return tf.keras.Model([inputs_noise, inputs_emb], [x, mu, logvar], name=\"generator\")\n",
    "\n",
    "def Discriminador():\n",
    "    tam_ini = 32\n",
    "    image_input = tf.keras.Input(shape=(BASE_SIZE, BASE_SIZE, 3))\n",
    "    inputs_emb = tf.keras.layers.Input(shape=(OUT_EMB,))\n",
    "    if projection_dims!=0:\n",
    "        emb = layers.Dense(projection_dims, use_bias=False)(inputs_emb) \n",
    "    else:\n",
    "        emb = inputs_emb \n",
    "        \n",
    "    x = layers.Conv2D(tam_ini, (4, 4), strides=(2, 2), padding='same')(image_input)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    #x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = layers.Conv2D(tam_ini*2, (4, 4), strides=(2, 2), padding='same')(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    #x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = layers.Conv2D(tam_ini*4, (4, 4), strides=(2, 2), padding='same')(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    #x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    x = layers.Conv2D(tam_ini*8, (4, 4), strides=(2, 2), padding='same')(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    #x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    # print(\"X shape:\",x.shape)\n",
    "    # x = layers.Conv2D(tam_ini*16, (4, 4), strides=(2, 2), padding='same')(x)\n",
    "    # x = layers.LeakyReLU()(x)\n",
    "    # x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "\n",
    "    #x = layers.Dropout(0.3)(x)\n",
    "    #x = layers.Conv2D(tam_ini*32, (4, 4), strides=(2, 2), padding='same')(x)\n",
    "    #x = layers.LeakyReLU()(x)\n",
    "    #x = layers.Dropout(0.3)(x)\n",
    "    # print(\"labels_input shape:\", labels_input.shape)\n",
    "    emb = layers.RepeatVector(16)(emb)\n",
    "    # print(\"After, reshape:\",emb)\n",
    "    emb = layers.Reshape((4,4, emb.shape[-1]))(emb)\n",
    "\n",
    "    x = tf.concat([x, emb], 3)\n",
    "    # print(\"After concat reshape:\",x.shape)\n",
    "    x = layers.Conv2D(1, (1, 1), strides=(1,1), padding='same')(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "    #print(\"sh:\",x.shape)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    #x = layers.Dense(100)(x)\n",
    "\n",
    "    output_score = layers.Dense(1,activation=\"sigmoid\")(x)\n",
    "\n",
    "    return tf.keras.Model([image_input, inputs_emb], output_score, name=\"discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44199984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(out_planes):\n",
    "    \"3x3 convolution with padding\"\n",
    "    return layers.Conv2D(out_planes, (3, 3), strides=(1, 1), padding='same')\n",
    "\n",
    "# Upsale the spatial size by a factor of 2\n",
    "def upBlock(out_planes):\n",
    "    block =  tf.keras.Sequential([\n",
    "        tf.keras.layers.UpSampling2D(size=(2, 2)),\n",
    "        conv3x3(out_planes*2),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        GatedLinearUnit2D()\n",
    "    ])\n",
    "    return block\n",
    "\n",
    "# Keep the spatial size\n",
    "def Block3x3_relu(in_planes, out_planes):\n",
    "    block =  tf.keras.Sequential([\n",
    "        conv3x3(out_planes*2),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        GatedLinearUnit2D()\n",
    "\n",
    "    ])\n",
    "    return block\n",
    "\n",
    "class ResBlock(layers.Layer):\n",
    "    def __init__(self, channel_num):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.block =tf.keras.Sequential([\n",
    "            conv3x3(channel_num * 2),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            conv3x3(channel_num),\n",
    "            tf.keras.layers.BatchNormalization()])\n",
    "\n",
    "    def call(self, x):\n",
    "        residual = x\n",
    "        out = self.block(x)\n",
    "        out += residual\n",
    "        return out\n",
    "def Generator():\n",
    "    tam_ini = 256\n",
    "    ca_net = cond_aug()\n",
    "\n",
    "    initializer = tf.random_normal_initializer(0., 0.05)\n",
    "    inputs_noise = tf.keras.layers.Input(shape=(NOISE_SIZE,))#output 100\n",
    "    inputs_emb = tf.keras.layers.Input(shape=(OUT_EMB,))\n",
    "    emb, mu, logvar = ca_net(inputs_emb)\n",
    "    x = tf.concat([inputs_noise, emb], 1)#output 200\n",
    "    x = layers.Dense(4*4*tam_ini*2, use_bias=False)(x) #l x por GLU que me divide el tamaño\n",
    "    x = GatedLinearUnit()(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = layers.Reshape((4,4, tam_ini))(x)\n",
    "\n",
    "    x = upBlock(tam_ini//2)(x)\n",
    "    x = upBlock(tam_ini//4)(x)\n",
    "    x = upBlock(tam_ini//8)(x)  \n",
    "    x = upBlock(tam_ini//16)(x)\n",
    "    #x = upBlock(tam_ini//32)(x)\n",
    "\n",
    "    x = conv3x3(3)(x)\n",
    "    x = tf.keras.activations.tanh(x)\n",
    "    return tf.keras.Model([inputs_noise, inputs_emb], [x, mu, logvar], name=\"generator\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7e3a719",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " cond_aug (Functional)          [(None, 100),        205200      ['input_3[0][0]']                \n",
      "                                 (None, 100),                                                     \n",
      "                                 (None, 100)]                                                     \n",
      "                                                                                                  \n",
      " tf.concat (TFOpLambda)         (None, 200)          0           ['input_2[0][0]',                \n",
      "                                                                  'cond_aug[0][0]']               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 8192)         1638400     ['tf.concat[0][0]']              \n",
      "                                                                                                  \n",
      " gated_linear_unit_1 (GatedLine  (None, 4096)        0           ['dense_1[0][0]']                \n",
      " arUnit)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 4096)        16384       ['gated_linear_unit_1[0][0]']    \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 4, 4, 256)    0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, 8, 8, 128)    591104      ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " sequential_1 (Sequential)      (None, 16, 16, 64)   148096      ['sequential[0][0]']             \n",
      "                                                                                                  \n",
      " sequential_2 (Sequential)      (None, 32, 32, 32)   37184       ['sequential_1[0][0]']           \n",
      "                                                                                                  \n",
      " sequential_3 (Sequential)      (None, 64, 64, 16)   9376        ['sequential_2[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 64, 64, 3)    435         ['sequential_3[0][0]']           \n",
      "                                                                                                  \n",
      " tf.math.tanh (TFOpLambda)      (None, 64, 64, 3)    0           ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,646,179\n",
      "Trainable params: 2,637,027\n",
      "Non-trainable params: 9,152\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator = Generator()\n",
    "generator.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04cc0df7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 64, 64, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 32, 32, 32)   1568        ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 32, 32, 32)   0           ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 16, 16, 64)   32832       ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 16, 16, 64)   0           ['conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 16, 16, 64)  256         ['leaky_re_lu_1[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 8, 8, 128)    131200      ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, 8, 8, 128)    0           ['conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 8, 8, 128)   512         ['leaky_re_lu_2[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 4, 4, 256)    524544      ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 128)          65536       ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)      (None, 4, 4, 256)    0           ['conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " repeat_vector (RepeatVector)   (None, 16, 128)      0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 4, 4, 256)   1024        ['leaky_re_lu_3[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 4, 4, 128)    0           ['repeat_vector[0][0]']          \n",
      "                                                                                                  \n",
      " tf.concat_1 (TFOpLambda)       (None, 4, 4, 384)    0           ['batch_normalization_7[0][0]',  \n",
      "                                                                  'reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 4, 4, 1)      385         ['tf.concat_1[0][0]']            \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)      (None, 4, 4, 1)      0           ['conv2d_9[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 4, 4, 1)     4           ['leaky_re_lu_4[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 16)           0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            17          ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 757,878\n",
      "Trainable params: 756,980\n",
      "Non-trainable params: 898\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = Discriminador()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf99c25",
   "metadata": {},
   "source": [
    "### Text encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7629c7",
   "metadata": {},
   "source": [
    "Aun no claro si conviene o no usar este embedding y entrenarlo junto con el discriminador y generador. Seria una parte de ambas redes."
   ]
  },
  {
   "cell_type": "raw",
   "id": "858e9b0b",
   "metadata": {},
   "source": [
    "num_projection_layers = 2\n",
    "projection_dims = 100\n",
    "dropout_rate = 0.2\n",
    "def get_text_enconder(num_projection_layers=2, projection_dims=100, dropout_rate=0.2):\n",
    "    inputs = layers.Input(shape=(512,), name=\"pesos de clip\")\n",
    "    projected_embeddings = layers.Dense(units=projection_dims)(inputs)\n",
    "    for _ in range(num_projection_layers):\n",
    "        x = tf.nn.gelu(projected_embeddings)\n",
    "        x = layers.Dense(projection_dims)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.Add()([projected_embeddings, x])\n",
    "        projected_embeddings = layers.LayerNormalization()(x)\n",
    "    return tf.keras.Model(inputs, projected_embeddings, name=\"text_encoder\")\n",
    "text_encoder = get_text_enconder(num_projection_layers, projection_dims, dropout_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb9412a",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d52e532",
   "metadata": {},
   "source": [
    "Labels discriminador:\n",
    "+ 0 Imagen falsa para el discriminador\n",
    "+ 0 Imagen verdadera pero mal texto \n",
    "+ 1 Verdadera imagen y texto asociado\n",
    "\n",
    "Labels generador:\n",
    "+ 1 Falsa imagen y generada con texto real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7e47fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def kl_divergence(mu, logvar):\n",
    "#    # Computes KL(p || q). Note that outside of this function, P is the target distribution\n",
    "#    # and Q is the approximating distribution. Inside of this function, P is just the\n",
    "#    # first argument and Q is the second.\n",
    "#    # I inefficiently iterate through all of the q and p values to\n",
    "#    # only compute the log when p_i > 0.\n",
    "#    KLD_element = tf.math.add(tf.math.pow(mu,2),(tf.math.exp(logvar)))\n",
    "#    KLD_element = tf.math.add(tf.math.add(tf.math.multiply(KLD_element,-1),1),logvar)\n",
    "#    KLD = tf.math.multiply(tf.math.reduce_mean(KLD_element),-0.5)\n",
    "#    return KLD\n",
    "\n",
    "def kl_divergence(mean, variance):\n",
    "    return -0.5 * tf.reduce_sum(1 + variance - tf.square(mean) - tf.exp(variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c59a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 300\n",
    "batch_size = 32\n",
    "learning_rate = 2e-4\n",
    "beta_1 = 0.5\n",
    "num_train_gen = 8 #por cada batch cuantas veces entrenamos el generador\n",
    "cuantas_imgs_vis = 3 #imagenes que vamos viendo al entrenar al final de cada epoca\n",
    "kl_divergence_loss_coeff = 1\n",
    "\n",
    "dataset = tf.data.Dataset.range(len(filenames)) \n",
    "dataset = dataset.shuffle(buffer_size=len(filenames)) # comment this line if you don't want to shuffle data\n",
    "dataset = dataset.batch(batch_size=batch_size, drop_remainder=True)\n",
    "dataset = dataset.repeat(epochs)\n",
    "prepare_caption = PREPARE_CAPTIONS(data_dir, split)\n",
    "filenames, captions = prepare_caption.load_text_data(data_dir, split)\n",
    "prepare_image = PREPARE_IMAGE(data_dir, split, captions, filenames, BASE_SIZE)\n",
    "\n",
    "discriminator = Discriminador()\n",
    "generator = Generator()\n",
    "model_clip = TFCLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "d_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1)\n",
    "g_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1)\n",
    "d_loss_fn = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.1)\n",
    "g_loss_fn = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.1)\n",
    "#kl = tf.keras.losses.KLDivergence()\n",
    "\n",
    "epoca = 0\n",
    "num_batch = 0\n",
    "when_end_epoch =int(len(filenames)/batch_size+1)\n",
    "print(\"EPOCA:\", epoca)\n",
    "\n",
    "disc_loss_tracker_batch = []\n",
    "gen_loss_tracker_batch = []\n",
    "kl_loss_tracker_batch = []\n",
    "disc_loss_tracker_epoch = []\n",
    "gen_loss_tracker_epoch = []\n",
    "kl_loss_tracker_epoch = []\n",
    "\n",
    "for batch in tqdm(dataset):\n",
    "    if num_batch!=0 and num_batch%when_end_epoch==0:\n",
    "        # random_latent_vectors = tf.random.uniform(shape=(cuantas_imgs_vis, NOISE_SIZE), minval=-1, maxval=1)\n",
    "        random_latent_vectors = tf.random.normal(shape=(cuantas_imgs_vis, NOISE_SIZE), mean=0.0, stddev=1.0, dtype=tf.dtypes.float32)\n",
    "        fake_images,  mu, logvar = generator([random_latent_vectors, caps_list_emb[:cuantas_imgs_vis]])\n",
    "        for title, img in zip(caps_list[:cuantas_imgs_vis], fake_images):\n",
    "            img = (img+1)/2\n",
    "            plt.figure(figsize=(3,3))\n",
    "            plt.imshow(img)\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(title)\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "        disc_loss_tracker_epoch.append(np.mean([disc_loss_tracker_batch]))\n",
    "        gen_loss_tracker_epoch.append(np.mean([gen_loss_tracker_batch]))\n",
    "        kl_loss_tracker_epoch.append(np.mean([kl_loss_tracker_batch]))\n",
    "\n",
    "        epoca+=1\n",
    "        print(\"EPOCA:\", epoca)\n",
    "        disc_loss_tracker_batch = []\n",
    "        gen_loss_tracker_batch = []\n",
    "        kl_loss_tracker_batch = []\n",
    "\n",
    "    # Cargo el batch y lo preparo ordenandolo en funcion de mas o menos palabras en el cap\n",
    "    # imgs_list, caps_list, cap_len_list, cls_id_list, key_list, wrong_caps_list, wrong_cap_len_list, wrong_cls_id_list = [], [], [], [], [], [], [], [] \n",
    "    imgs_list, caps_list, cls_id_list, key_list, wrong_caps_list, wrong_cls_id_list = [], [], [], [], [], []\n",
    "\n",
    "    for ind in batch:\n",
    "        imgs, caps, cls_id, key, wrong_caps, wrong_cls_id = prepare_image.__getitem__(ind)\n",
    "        imgs_list.append(imgs)\n",
    "        caps_list.append(caps)\n",
    "        #cap_len_list.append(cap_len)\n",
    "        cls_id_list.append(cls_id)\n",
    "        key_list.append(key)\n",
    "        wrong_caps_list.append(wrong_caps)\n",
    "        #wrong_cap_len_list.append(wrong_cap_len)\n",
    "        wrong_cls_id_list.append(wrong_cls_id)\n",
    "    # ordeno las listas de mas a menos palabras\n",
    "    #sorted_cap_indices = tf.argsort(cap_len_list,direction='DESCENDING').numpy()\n",
    "    #imgs_list = np.array(imgs_list)[sorted_cap_indices]\n",
    "    #caps_list = np.array(caps_list)[sorted_cap_indices]\n",
    "    #cls_id_list = np.array(cls_id_list)[sorted_cap_indices]\n",
    "    #key_list = [key_list[i] for i in sorted_cap_indices]\n",
    "    #sorted_cap_indices = tf.argsort(wrong_cap_len_list,direction='DESCENDING').numpy()\n",
    "    #wrong_caps_list = np.array(wrong_caps_list)[sorted_cap_indices]\n",
    "    #wrong_cap_len_list = np.array(wrong_cap_len_list)[sorted_cap_indices]\n",
    "    #wrong_cls_id_list = np.array(wrong_cls_id_list)[sorted_cap_indices]\n",
    "    \n",
    "    caps_list_token = tokenizer(caps_list, padding='max_length', truncation = True, max_length = WORDS_NUM, return_tensors=\"tf\")\n",
    "    wrong_caps_list_token = tokenizer(wrong_caps_list, padding='max_length', truncation = True, max_length = WORDS_NUM, return_tensors=\"tf\")\n",
    "    caps_list_emb = model_clip(caps_list_token).pooler_output\n",
    "    wrong_caps_list_emb = model_clip(wrong_caps_list_token).pooler_output\n",
    "    \n",
    "    # caps_list_emb = tf.math.divide(tf.subtract(caps_list_emb, tf.reduce_min(caps_list_emb)), tf.subtract(tf.reduce_max(caps_list_emb), tf.reduce_min(caps_list_emb)))\n",
    "    # wrong_caps_list_emb = tf.math.divide(tf.subtract(wrong_caps_list_emb, tf.reduce_min(wrong_caps_list_emb)), tf.subtract(tf.reduce_max(wrong_caps_list_emb), tf.reduce_min(wrong_caps_list_emb)))\n",
    "    # random_latent_vectors = tf.random.uniform(shape=(batch_size, NOISE_SIZE), minval=-1, maxval=1)\n",
    "    random_latent_vectors = tf.random.normal(shape=(batch_size, NOISE_SIZE), mean=0.0, stddev=1.0, dtype=tf.dtypes.float32)\n",
    "\n",
    "    \n",
    "    # TRAIN DISCRIMINATOR\n",
    "    \n",
    "    #indices = tf.range(start=0, limit=batch_size*3, dtype=tf.int32) #por 3\n",
    "    #shuffled_indices = tf.random.shuffle(indices)\n",
    "    labels = tf.concat([tf.zeros((batch_size, 1)), tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0)\n",
    "    #labels = tf.gather(labels, shuffled_indices)\n",
    "    loss_disc = 0\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        embedings = tf.concat([caps_list_emb, caps_list_emb, wrong_caps_list_emb], axis=0)\n",
    "        generated_images, mu, logvar = generator([random_latent_vectors, caps_list_emb])\n",
    "        combined_images = tf.concat([generated_images, imgs_list, imgs_list], axis=0)\n",
    "        #embedings = tf.gather(embedings, shuffled_indices)\n",
    "        #combined_images = tf.gather(combined_images, shuffled_indices)\n",
    "        for i in range(3):\n",
    "            predictions = discriminator([combined_images[i*batch_size:(i+1)*batch_size], embedings[i*batch_size:(i+1)*batch_size]])\n",
    "            d_loss = d_loss_fn(labels[i*batch_size:(i+1)*batch_size], predictions)\n",
    "            loss_disc +=d_loss\n",
    "            grads = tape.gradient(d_loss, discriminator.trainable_weights)            \n",
    "            d_optimizer.apply_gradients((grad, var) for (grad, var) in zip(grads, discriminator.trainable_variables) if grad is not None)\n",
    "            \n",
    "    del tape #Por ser persistente\n",
    "\n",
    "    #del tape #Por ser persistente\n",
    "    disc_loss_tracker_batch.append(loss_disc/3)\n",
    "\n",
    "    misleading_labels = tf.ones((batch_size, 1))    \n",
    "    loss_gen = 0\n",
    "    loss_kl = 0\n",
    "    for _ in range(num_train_gen):\n",
    "        # random_latent_vectors = tf.random.uniform(shape=(batch_size, NOISE_SIZE), minval=-1, maxval=1)\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, NOISE_SIZE), mean=0.0, stddev=1.0, dtype=tf.dtypes.float32)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            fake_images, mu, logvar = generator([random_latent_vectors, caps_list_emb])\n",
    "            predictions = discriminator([fake_images, caps_list_emb])\n",
    "            g_loss = g_loss_fn(misleading_labels, predictions) # añadir termino de divergencia KL\n",
    "            kl_loss = kl_divergence_loss_coeff*kl_divergence(mu, logvar) #kl(misleading_labels, predictions).numpy()\n",
    "            g_loss += kl_loss\n",
    "        loss_kl += kl_loss\n",
    "        loss_gen += g_loss \n",
    "        grads = tape.gradient(g_loss, generator.trainable_weights)\n",
    "        g_optimizer.apply_gradients(zip(grads, generator.trainable_weights))\n",
    "    kl_loss_tracker_batch.append(loss_kl/num_train_gen)\n",
    "    gen_loss_tracker_batch.append(loss_gen/num_train_gen)\n",
    "    if num_batch%(when_end_epoch//10)==0:\n",
    "        print(\"Loss DISC:\", np.mean([disc_loss_tracker_batch]), \"Loss GEN:\", np.mean([gen_loss_tracker_batch]), \"Loss KL:\",np.mean(kl_loss_tracker_batch))\n",
    "        \n",
    "    num_batch += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51595672",
   "metadata": {},
   "source": [
    "# Train solo gan de imagenes, no texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800cac88",
   "metadata": {},
   "source": [
    "### Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "0956498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_dims = 128\n",
    "def upsample(filters, size, apply_dropout=False,last = False):\n",
    "    initializer = tf.random_normal_initializer(0., 0.05)\n",
    "    result = tf.keras.Sequential()\n",
    "    if last:\n",
    "        result.add(tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
    "                                        padding='same',\n",
    "                                        kernel_initializer=initializer,\n",
    "                                        use_bias=False, activation='tanh'))\n",
    "    else:\n",
    "        result.add(\n",
    "        tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
    "                                        padding='same',\n",
    "                                        kernel_initializer=initializer,\n",
    "                                        use_bias=False))\n",
    "        result.add(tf.keras.layers.BatchNormalization())\n",
    "        if apply_dropout:\n",
    "            result.add(tf.keras.layers.Dropout(0.1))\n",
    "        result.add(tf.keras.layers.LeakyReLU(0.2))\n",
    "    return result\n",
    "\n",
    "def Generator():\n",
    "    tam_ini = 512\n",
    "    \n",
    "    initializer = tf.random_normal_initializer(0., 0.05)\n",
    "    inputs_noise = tf.keras.layers.Input(shape=(NOISE_SIZE,))#output 100\n",
    "\n",
    "    x = layers.Dense(4*4*tam_ini, use_bias=False)(inputs_noise)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "    x = layers.Reshape((4,4, tam_ini))(x)\n",
    "    x = upsample(tam_ini//2, 3, apply_dropout=False)(x)\n",
    "    x = upsample(tam_ini//4, 3, apply_dropout=False)(x)\n",
    "    x = upsample(tam_ini//8, 3, apply_dropout=False)(x)\n",
    "    #x = upsample(tam_ini//16, 4)(x) \n",
    "    x = upsample(3, 3,last = True)(x)\n",
    "    return tf.keras.Model(inputs_noise, x, name=\"generator\")\n",
    "\n",
    "def Discriminador():\n",
    "    tam_ini = 64\n",
    "    image_input = tf.keras.Input(shape=(BASE_SIZE, BASE_SIZE, 3))\n",
    "        \n",
    "    x = layers.Conv2D(tam_ini, (4, 4), strides=(2, 2), padding='same')(image_input)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    #x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = layers.Conv2D(tam_ini*2, (4, 4), strides=(2, 2), padding='same')(x)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "    #x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = layers.Conv2D(tam_ini*4, (4, 4), strides=(2, 2), padding='same')(x)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "   # x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    x = layers.Conv2D(tam_ini*8, (4, 4), strides=(2, 2), padding='same')(x)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "    #x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    # print(\"X shape:\",x.shape)\n",
    "    # x = layers.Conv2D(tam_ini*16, (4, 4), strides=(2, 2), padding='same')(x)\n",
    "    # x = layers.LeakyReLU()(x)\n",
    "    # x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "\n",
    "    #x = layers.Dropout(0.3)(x)\n",
    "    #x = layers.Conv2D(tam_ini*32, (4, 4), strides=(2, 2), padding='same')(x)\n",
    "    #x = layers.LeakyReLU()(x)\n",
    "    #x = layers.Dropout(0.3)(x)\n",
    "    # print(\"labels_input shape:\", labels_input.shape)\n",
    "\n",
    "    # print(\"After concat reshape:\",x.shape)\n",
    "    x = layers.Conv2D(tam_ini*16, (1, 1), strides=(1,1), padding='same')(x)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "   # x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "    #print(\"sh:\",x.shape)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    #x = layers.Dense(100)(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "\n",
    "    # output_score = layers.Dense(1,activation=\"sigmoid\")(x)\n",
    "    output_score = layers.Dense(1)(x)#,activation=\"sigmoid\")(x)\n",
    "    return tf.keras.Model(image_input, output_score, name=\"discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "f93ecd7e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_95 (InputLayer)       [(None, 100)]             0         \n",
      "                                                                 \n",
      " dense_92 (Dense)            (None, 8192)              819200    \n",
      "                                                                 \n",
      " batch_normalization_169 (Ba  (None, 8192)             32768     \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_278 (LeakyReLU)  (None, 8192)             0         \n",
      "                                                                 \n",
      " reshape_36 (Reshape)        (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " sequential_139 (Sequential)  (None, 8, 8, 256)        1180672   \n",
      "                                                                 \n",
      " sequential_140 (Sequential)  (None, 16, 16, 128)      295424    \n",
      "                                                                 \n",
      " sequential_141 (Sequential)  (None, 32, 32, 64)       73984     \n",
      "                                                                 \n",
      " sequential_142 (Sequential)  (None, 64, 64, 3)        1728      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,403,776\n",
      "Trainable params: 2,386,496\n",
      "Non-trainable params: 17,280\n",
      "_________________________________________________________________\n",
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_96 (InputLayer)       [(None, 64, 64, 3)]       0         \n",
      "                                                                 \n",
      " conv2d_180 (Conv2D)         (None, 32, 32, 64)        3136      \n",
      "                                                                 \n",
      " leaky_re_lu_282 (LeakyReLU)  (None, 32, 32, 64)       0         \n",
      "                                                                 \n",
      " conv2d_181 (Conv2D)         (None, 16, 16, 128)       131200    \n",
      "                                                                 \n",
      " leaky_re_lu_283 (LeakyReLU)  (None, 16, 16, 128)      0         \n",
      "                                                                 \n",
      " dropout_120 (Dropout)       (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " conv2d_182 (Conv2D)         (None, 8, 8, 256)         524544    \n",
      "                                                                 \n",
      " leaky_re_lu_284 (LeakyReLU)  (None, 8, 8, 256)        0         \n",
      "                                                                 \n",
      " dropout_121 (Dropout)       (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " conv2d_183 (Conv2D)         (None, 4, 4, 512)         2097664   \n",
      "                                                                 \n",
      " leaky_re_lu_285 (LeakyReLU)  (None, 4, 4, 512)        0         \n",
      "                                                                 \n",
      " dropout_122 (Dropout)       (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " conv2d_184 (Conv2D)         (None, 4, 4, 1024)        525312    \n",
      "                                                                 \n",
      " leaky_re_lu_286 (LeakyReLU)  (None, 4, 4, 1024)       0         \n",
      "                                                                 \n",
      " flatten_34 (Flatten)        (None, 16384)             0         \n",
      "                                                                 \n",
      " dropout_123 (Dropout)       (None, 16384)             0         \n",
      "                                                                 \n",
      " dense_93 (Dense)            (None, 1)                 16385     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,298,241\n",
      "Trainable params: 3,298,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator = Generator()\n",
    "generator.summary()\n",
    "discriminator = Discriminador()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5962264",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "f9e009ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_305 (Conv2D)         (None, 32, 32, 64)        4864      \n",
      "                                                                 \n",
      " leaky_re_lu_446 (LeakyReLU)  (None, 32, 32, 64)       0         \n",
      "                                                                 \n",
      " conv2d_306 (Conv2D)         (None, 16, 16, 128)       204928    \n",
      "                                                                 \n",
      " batch_normalization_271 (Ba  (None, 16, 16, 128)      512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_447 (LeakyReLU)  (None, 16, 16, 128)      0         \n",
      "                                                                 \n",
      " conv2d_307 (Conv2D)         (None, 8, 8, 256)         819456    \n",
      "                                                                 \n",
      " batch_normalization_272 (Ba  (None, 8, 8, 256)        1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_448 (LeakyReLU)  (None, 8, 8, 256)        0         \n",
      "                                                                 \n",
      " conv2d_308 (Conv2D)         (None, 4, 4, 512)         3277312   \n",
      "                                                                 \n",
      " batch_normalization_273 (Ba  (None, 4, 4, 512)        2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_449 (LeakyReLU)  (None, 4, 4, 512)        0         \n",
      "                                                                 \n",
      " flatten_38 (Flatten)        (None, 8192)              0         \n",
      "                                                                 \n",
      " dense_139 (Dense)           (None, 1)                 8193      \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,318,337\n",
      "Trainable params: 4,316,545\n",
      "Non-trainable params: 1,792\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "\n",
    "# Create the discriminator\n",
    "tam_ini = 64\n",
    "discriminator = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(BASE_SIZE, BASE_SIZE, 3)),\n",
    "        layers.Conv2D(tam_ini, (5, 5), strides=(2, 2), padding=\"same\",kernel_initializer='glorot_uniform'),\n",
    "       # layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(tam_ini*2, (5, 5), strides=(2, 2), padding=\"same\",kernel_initializer='glorot_uniform'),\n",
    "        layers.BatchNormalization(momentum=0.5),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(tam_ini*4, (5, 5), strides=(2, 2), padding=\"same\",kernel_initializer='glorot_uniform'),\n",
    "        layers.BatchNormalization(momentum=0.5),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(tam_ini*8, (5, 5), strides=(2, 2), padding=\"same\",kernel_initializer='glorot_uniform'),\n",
    "        layers.BatchNormalization(momentum=0.5),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Flatten(),\n",
    "        #layers.GlobalMaxPooling2D(),\n",
    "        layers.Dense(1),\n",
    "        layers.Activation('sigmoid')\n",
    "    ],\n",
    "    name=\"discriminator\",\n",
    ")\n",
    "\n",
    "discriminator.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "cb2bd5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_143 (InputLayer)      [(None, 128)]             0         \n",
      "                                                                 \n",
      " dense_140 (Dense)           (None, 8192)              1048576   \n",
      "                                                                 \n",
      " reshape_61 (Reshape)        (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " batch_normalization_274 (Ba  (None, 4, 4, 512)        2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " sequential_183 (Sequential)  (None, 8, 8, 256)        3278080   \n",
      "                                                                 \n",
      " sequential_184 (Sequential)  (None, 16, 16, 128)      819840    \n",
      "                                                                 \n",
      " sequential_185 (Sequential)  (None, 32, 32, 64)       205120    \n",
      "                                                                 \n",
      " sequential_186 (Sequential)  (None, 64, 64, 32)       51360     \n",
      "                                                                 \n",
      " conv2d_313 (Conv2D)         (None, 64, 64, 3)         2403      \n",
      "                                                                 \n",
      " tf.math.tanh_10 (TFOpLambda  (None, 64, 64, 3)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,407,427\n",
      "Trainable params: 5,405,443\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def conv3x3(out_planes):\n",
    "    \"3x3 convolution with padding\"\n",
    "    return layers.Conv2D(out_planes, (5, 5), strides=(1, 1), padding='same', kernel_initializer='glorot_uniform')\n",
    "\n",
    "# Upsale the spatial size by a factor of 2\n",
    "def upBlock(out_planes):\n",
    "    block =  tf.keras.Sequential([\n",
    "        tf.keras.layers.UpSampling2D(size=(2, 2)),\n",
    "        conv3x3(out_planes),\n",
    "        #layers.Dropout(0.2),\n",
    "        tf.keras.layers.BatchNormalization(momentum=0.5),\n",
    "        layers.Activation(\"relu\"),\n",
    "\n",
    "    ])\n",
    "    return block\n",
    "\n",
    "def Generator():\n",
    "    tam_ini = 512\n",
    "    inputs_noise = tf.keras.layers.Input(shape=(NOISE_SIZE,))#output 100\n",
    "    x = layers.Dense(4*4*tam_ini, use_bias=False, kernel_initializer='glorot_uniform')(inputs_noise) #l x por GLU que me divide el tamaño\n",
    "    #x = layers.Dropout(0.2)(x)\n",
    "   # \n",
    "    x = layers.Reshape((4,4, tam_ini))(x)\n",
    "    x = layers.BatchNormalization(momentum=0.5)(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = upBlock(tam_ini//2)(x)\n",
    "    x = upBlock(tam_ini//4)(x)\n",
    "    x = upBlock(tam_ini//8)(x)  \n",
    "    x = upBlock(tam_ini//16)(x)\n",
    "    #x = upBlock(tam_ini//32)(x)\n",
    "    x = conv3x3(3)(x)\n",
    "    x = tf.keras.activations.tanh(x)\n",
    "    return tf.keras.Model(inputs_noise, x,  name=\"generator\")\n",
    "generator = Generator()\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "9e97aaf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load filenames from: ../data/birds/train/filenames.pickle (8855)\n",
      "Load filenames from: ../data/birds/train/filenames.pickle (8855)\n",
      "filepath ../data/birds\\captions.pickle\n",
      "Load from:  ../data/birds\\captions.pickle\n",
      "Total filenames:  11788 001.Black_footed_Albatross/Black_Footed_Albatross_0046_18.jpg\n"
     ]
    }
   ],
   "source": [
    "def discriminator_loss(real_img, fake_img):\n",
    "    real_loss = tf.reduce_mean(real_img)\n",
    "    fake_loss = tf.reduce_mean(fake_img)\n",
    "    return fake_loss - real_loss\n",
    "\n",
    "\n",
    "# Define the loss functions for the generator.\n",
    "def generator_loss(fake_img):\n",
    "    return -tf.reduce_mean(fake_img)\n",
    "\n",
    "def gradient_penalty(batch_size, real_images, fake_images):\n",
    "    \"\"\" Calculates the gradient penalty.\n",
    "\n",
    "    This loss is calculated on an interpolated image\n",
    "    and added to the discriminator loss.\n",
    "    \"\"\"\n",
    "    # Get the interpolated image\n",
    "    alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "    diff = fake_images - real_images\n",
    "    interpolated = real_images + alpha * diff\n",
    "   \n",
    "    with tf.GradientTape() as gp_tape:\n",
    "        gp_tape.watch(interpolated)\n",
    "        # 1. Get the discriminator output for this interpolated image.\n",
    "        pred = discriminator(interpolated, training=True)\n",
    "\n",
    "    # 2. Calculate the gradients w.r.t to this interpolated image.\n",
    "    grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "    # 3. Calculate the norm of the gradients.\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
    "    gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "    return gp\n",
    "\n",
    "\n",
    "epochs = 300\n",
    "batch_size = 64\n",
    "learning_rate = 2e-4\n",
    "beta_1 = 0.5\n",
    "beta_2 = 0.9\n",
    "num_train_gen = 1 #por cada batch cuantas veces entrenamos el generador\n",
    "num_train_disc = 1\n",
    "cuantas_imgs_vis = 3 #imagenes que vamos viendo al entrenar al final de cada epoca\n",
    "kl_divergence_loss_coeff = 1\n",
    "\n",
    "dataset = tf.data.Dataset.range(len(filenames)) \n",
    "dataset = dataset.shuffle(buffer_size=len(filenames)) # comment this line if you don't want to shuffle data\n",
    "dataset = dataset.batch(batch_size=batch_size, drop_remainder=True)\n",
    "dataset = dataset.repeat(epochs)\n",
    "prepare_caption = PREPARE_CAPTIONS(data_dir, split)\n",
    "filenames, captions = prepare_caption.load_text_data(data_dir, split)\n",
    "prepare_image = PREPARE_IMAGE(data_dir, split, captions, filenames, BASE_SIZE)\n",
    "\n",
    "#discriminator = Discriminador()\n",
    "#generator = Generator()\n",
    "#model_clip = TFCLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "#tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "d_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2)\n",
    "g_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2)\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy()#from_logits=True)\n",
    "\n",
    "def train_step(real_images):\n",
    "    # Sample random points in the latent space\n",
    "    loss_disc = 0\n",
    "    real_images = tf.convert_to_tensor(real_images)\n",
    "    for _ in range(num_train_disc):\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, NOISE_SIZE))\n",
    "        # Decode them to fake images\n",
    "        generated_images = generator(random_latent_vectors)\n",
    "\n",
    "        labels = tf.zeros((batch_size, 1))\n",
    "        # Add random noise to the labels - important trick!\n",
    "        labels += 0.2 * tf.random.uniform(labels.shape)\n",
    "    \n",
    "        # Train the discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = discriminator(generated_images)\n",
    "            #predictions_real = discriminator(real_images)\n",
    "            d_loss = loss_fn(labels, predictions)\n",
    "            #gp = gradient_penalty(batch_size, real_images, generated_images)\n",
    "            #d_loss = discriminator_loss(predictions_real, predictions_fake)+10*gp\n",
    "            loss_disc +=d_loss\n",
    "        grads = tape.gradient(d_loss, discriminator.trainable_weights)\n",
    "        d_optimizer.apply_gradients(zip(grads, discriminator.trainable_weights))\n",
    "        \n",
    "        labels = tf.ones((batch_size, 1))\n",
    "        # Add random noise to the labels - important trick!\n",
    "        labels -= 0.2 * tf.random.uniform(labels.shape)\n",
    "        # Train the discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = discriminator(tf.convert_to_tensor(real_images))\n",
    "            d_loss = loss_fn(labels, predictions)\n",
    "            loss_disc +=d_loss\n",
    "        grads = tape.gradient(d_loss, discriminator.trainable_weights)\n",
    "        d_optimizer.apply_gradients(zip(grads, discriminator.trainable_weights))\n",
    "    \n",
    "    # Sample random points in the latent space\n",
    "    loss_gen = 0\n",
    "    misleading_labels = tf.ones((2*batch_size, 1))\n",
    "\n",
    "    for _ in range(num_train_gen):\n",
    "        random_latent_vectors = tf.random.normal(shape=(2*batch_size, NOISE_SIZE))\n",
    "        # Assemble labels that say \"all real images\"\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = discriminator(generator(random_latent_vectors))\n",
    "            g_loss = loss_fn(misleading_labels, predictions)\n",
    "            #g_loss = generator_loss(predictions)\n",
    "            loss_gen += g_loss \n",
    "        grads = tape.gradient(g_loss, generator.trainable_weights)\n",
    "        g_optimizer.apply_gradients(zip(grads, generator.trainable_weights))\n",
    "    return loss_disc, loss_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c792fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoca = 0\n",
    "num_batch = 0\n",
    "when_end_epoch =int(len(filenames)/batch_size+1)\n",
    "print(\"EPOCA:\", epoca)\n",
    "\n",
    "disc_loss_tracker_batch = []\n",
    "gen_loss_tracker_batch = []\n",
    "#kl_loss_tracker_batch = []\n",
    "disc_loss_tracker_epoch = []\n",
    "gen_loss_tracker_epoch = []\n",
    "#kl_loss_tracker_epoch = []\n",
    "\n",
    "for batch in tqdm(dataset):\n",
    "    if num_batch!=0 and num_batch%when_end_epoch==0:\n",
    "        # random_latent_vectors = tf.random.uniform(shape=(cuantas_imgs_vis, NOISE_SIZE), minval=-1, maxval=1)\n",
    "        random_latent_vectors = tf.random.normal(shape=(cuantas_imgs_vis, NOISE_SIZE), mean=0.0, stddev=1.0, dtype=tf.dtypes.float32)\n",
    "        fake_images = generator([random_latent_vectors])\n",
    "        for title, img in zip(caps_list[:cuantas_imgs_vis], fake_images):\n",
    "            img = (img+1)/2\n",
    "            plt.figure(figsize=(3,3))\n",
    "            plt.imshow(img)\n",
    "            plt.axis(\"off\")\n",
    "            #plt.title(title)\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "        disc_loss_tracker_epoch.append(np.mean([disc_loss_tracker_batch]))\n",
    "        gen_loss_tracker_epoch.append(np.mean([gen_loss_tracker_batch]))\n",
    "        #kl_loss_tracker_epoch.append(np.mean([kl_loss_tracker_batch]))\n",
    "\n",
    "        epoca+=1\n",
    "        print(\"EPOCA:\", epoca)\n",
    "        disc_loss_tracker_batch = []\n",
    "        gen_loss_tracker_batch = []\n",
    "        #kl_loss_tracker_batch = []\n",
    "\n",
    "    # Cargo el batch y lo preparo ordenandolo en funcion de mas o menos palabras en el cap\n",
    "    # imgs_list, caps_list, cap_len_list, cls_id_list, key_list, wrong_caps_list, wrong_cap_len_list, wrong_cls_id_list = [], [], [], [], [], [], [], [] \n",
    "    imgs_list, caps_list, cls_id_list, key_list, wrong_caps_list, wrong_cls_id_list = [], [], [], [], [], []\n",
    "\n",
    "    for ind in batch:\n",
    "        imgs, caps, cls_id, key, wrong_caps, wrong_cls_id = prepare_image.__getitem__(ind)\n",
    "        imgs_list.append(imgs)\n",
    "        caps_list.append(caps)\n",
    "\n",
    "    loss_disc, loss_gen = train_step(imgs_list)\n",
    "    disc_loss_tracker_batch.append(loss_disc/num_train_disc)\n",
    "    gen_loss_tracker_batch.append(loss_gen/num_train_gen)\n",
    "    if num_batch%(when_end_epoch//10)==0:\n",
    "        print(\"Loss DISC:\", np.mean([disc_loss_tracker_batch]), \"Loss GEN:\", np.mean([gen_loss_tracker_batch]))\n",
    "        \n",
    "    num_batch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "ff8afa8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (20,8))\n",
    "plt.plot(disc_loss_tracker_epoch, label = \"discriminador\")\n",
    "plt.plot(gen_loss_tracker_epoch, label=\"generator\")\n",
    "plt.legend()\n",
    "plt.title(\"Losses disc y generador\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1454961",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_latent_vectors = tf.random.normal(shape=(32, NOISE_SIZE), mean=0.0, stddev=1.0, dtype=tf.dtypes.float32)\n",
    "fake_images = generator([random_latent_vectors])\n",
    "n_row = 4\n",
    "n_col = 8\n",
    "_, axs = plt.subplots(n_row, n_col, figsize=(20,10))\n",
    "axs = axs.flatten()\n",
    "for img, ax in zip(fake_images, axs):\n",
    "    ax.imshow((img+1)/2)\n",
    "    ax.axis('off')\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96937c7b",
   "metadata": {},
   "source": [
    "# Train solo imágenes con aumento adaptativo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "321d52c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resolution of Kernel Inception Distance measurement, see related section\n",
    "kid_image_size = 75\n",
    "padding = 0.25\n",
    "\n",
    "# adaptive discriminator augmentation\n",
    "max_translation = 0.125\n",
    "max_rotation = 0.125\n",
    "max_zoom = 0.25\n",
    "target_accuracy = 0.85\n",
    "integration_steps = 1000\n",
    "\n",
    "# architecture\n",
    "noise_size = 128\n",
    "depth = 4\n",
    "width = 128\n",
    "leaky_relu_slope = 0.2\n",
    "dropout_rate = 0.4\n",
    "\n",
    "# optimization\n",
    "batch_size = 64\n",
    "learning_rate = 2e-4\n",
    "beta_1 = 0.5  # not using the default value of 0.9 is important\n",
    "ema = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756d6a50",
   "metadata": {},
   "source": [
    "### KID \n",
    "\n",
    "Kernel Inception Distance (KID) was proposed as a replacement for the popular Frechet Inception Distance (FID) metric for measuring image generation quality. Both metrics measure the difference in the generated and training distributions in the representation space of an InceptionV3 network pretrained on ImageNet.\n",
    "\n",
    "According to the paper, KID was proposed because FID has no unbiased estimator, its expected value is higher when it is measured on fewer images. KID is more suitable for small datasets because its expected value does not depend on the number of samples it is measured on. In my experience it is also computationally lighter, numerically more stable, and simpler to implement because it can be estimated in a per-batch manner.\n",
    "\n",
    "In this example, the images are evaluated at the minimal possible resolution of the Inception network (75x75 instead of 299x299), and the metric is only measured on the validation set for computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12a61be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KID(keras.metrics.Metric):\n",
    "    def __init__(self, name=\"kid\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "\n",
    "        # KID is estimated per batch and is averaged across batches\n",
    "        self.kid_tracker = keras.metrics.Mean()\n",
    "\n",
    "        # a pretrained InceptionV3 is used without its classification layer\n",
    "        # transform the pixel values to the 0-255 range, then use the same\n",
    "        # preprocessing as during pretraining\n",
    "        self.encoder = keras.Sequential(\n",
    "            [\n",
    "                layers.InputLayer(input_shape=(BASE_SIZE, BASE_SIZE, 3)),\n",
    "                layers.Resizing(height=kid_image_size, width=kid_image_size),\n",
    "                layers.Lambda(keras.applications.inception_v3.preprocess_input),\n",
    "                keras.applications.InceptionV3(\n",
    "                    include_top=False,\n",
    "                    input_shape=(kid_image_size, kid_image_size, 3),\n",
    "                    weights=\"imagenet\",\n",
    "                ),\n",
    "                layers.GlobalAveragePooling2D(),\n",
    "            ],\n",
    "            name=\"inception_encoder\",\n",
    "        )\n",
    "\n",
    "    def polynomial_kernel(self, features_1, features_2):\n",
    "        feature_dimensions = tf.cast(tf.shape(features_1)[1], dtype=tf.float32)\n",
    "        return (features_1 @ tf.transpose(features_2) / feature_dimensions + 1.0) ** 3.0\n",
    "\n",
    "    def update_state(self, real_images, generated_images, sample_weight=None):\n",
    "        real_features = self.encoder(real_images, training=False)\n",
    "        generated_features = self.encoder(generated_images, training=False)\n",
    "\n",
    "        # compute polynomial kernels using the two sets of features\n",
    "        kernel_real = self.polynomial_kernel(real_features, real_features)\n",
    "        kernel_generated = self.polynomial_kernel(\n",
    "            generated_features, generated_features\n",
    "        )\n",
    "        kernel_cross = self.polynomial_kernel(real_features, generated_features)\n",
    "\n",
    "        # estimate the squared maximum mean discrepancy using the average kernel values\n",
    "        batch_size = tf.shape(real_features)[0]\n",
    "        batch_size_f = tf.cast(batch_size, dtype=tf.float32)\n",
    "        mean_kernel_real = tf.reduce_sum(kernel_real * (1.0 - tf.eye(batch_size))) / (\n",
    "            batch_size_f * (batch_size_f - 1.0)\n",
    "        )\n",
    "        mean_kernel_generated = tf.reduce_sum(\n",
    "            kernel_generated * (1.0 - tf.eye(batch_size))\n",
    "        ) / (batch_size_f * (batch_size_f - 1.0))\n",
    "        mean_kernel_cross = tf.reduce_mean(kernel_cross)\n",
    "        kid = mean_kernel_real + mean_kernel_generated - 2.0 * mean_kernel_cross\n",
    "\n",
    "        # update the average KID estimate\n",
    "        self.kid_tracker.update_state(kid)\n",
    "\n",
    "    def result(self):\n",
    "        return self.kid_tracker.result()\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.kid_tracker.reset_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409882ab",
   "metadata": {},
   "source": [
    "### Adaptive discriminator augmentation\n",
    "\n",
    "The authors of StyleGAN2-ADA propose to change the augmentation probability adaptively during training. Though it is explained differently in the paper, they use integral control on the augmentation probability to keep the discriminator's accuracy on real images close to a target value. Note, that their controlled variable is actually the average sign of the discriminator logits (r_t in the paper), which corresponds to 2 * accuracy - 1.\n",
    "\n",
    "This method requires two hyperparameters:\n",
    "\n",
    "+ target_accuracy: the target value for the discriminator's accuracy on real images. I recommend selecting its value from the 80-90% range.\n",
    "+ integration_steps: the number of update steps required for an accuracy error of 100% to transform into an augmentation probability increase of 100%. To give an intuition, this defines how slowly the augmentation probability is changed. I recommend setting this to a relatively high value (1000 in this case) so that the augmentation strength is only adjusted slowly.\n",
    "\n",
    "The main motivation for this procedure is that the optimal value of the target accuracy is similar across different dataset sizes (see figure 4 and 5 in the paper), so it does not have to be retuned, because the process automatically applies stronger data augmentation when it is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e58c7435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"hard sigmoid\", useful for binary accuracy calculation from logits\n",
    "def step(values):\n",
    "    # negative values -> 0.0, positive values -> 1.0\n",
    "    return tf.math.round(values)\n",
    "    # return 0.5 * (1.0 + tf.sign(values))\n",
    "\n",
    "\n",
    "# augments images with a probability that is dynamically updated during training\n",
    "class AdaptiveAugmenter(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # stores the current probability of an image being augmented\n",
    "        self.probability = tf.Variable(0.0)\n",
    "\n",
    "        # the corresponding augmentation names from the paper are shown above each layer\n",
    "        # the authors show (see figure 4), that the blitting and geometric augmentations\n",
    "        # are the most helpful in the low-data regime\n",
    "        self.augmenter = keras.Sequential(\n",
    "            [\n",
    "                layers.InputLayer(input_shape=(BASE_SIZE, BASE_SIZE, 3)),\n",
    "                # blitting/x-flip:\n",
    "                # blitting/integer translation:\n",
    "                layers.RandomTranslation(\n",
    "                    height_factor=max_translation,\n",
    "                    width_factor=max_translation,\n",
    "                    interpolation=\"nearest\",\n",
    "                ),\n",
    "                # geometric/rotation:\n",
    "                layers.RandomRotation(factor=max_rotation),\n",
    "                # geometric/isotropic and anisotropic scaling:\n",
    "                layers.RandomZoom(\n",
    "                    height_factor=(-max_zoom, 0.0), width_factor=(-max_zoom, 0.0)\n",
    "                ),\n",
    "            ],\n",
    "            name=\"adaptive_augmenter\",\n",
    "        )\n",
    "\n",
    "    def call(self, images, training):\n",
    "        if training:\n",
    "            augmented_images = self.augmenter(images, training)\n",
    "\n",
    "            # during training either the original or the augmented images are selected\n",
    "            # based on self.probability\n",
    "            augmentation_values = tf.random.uniform(\n",
    "                shape=(batch_size, 1, 1, 1), minval=0.0, maxval=1.0\n",
    "            )\n",
    "            augmentation_bools = tf.math.less(augmentation_values, self.probability)\n",
    "\n",
    "            images = tf.where(augmentation_bools, augmented_images, images)\n",
    "        return images\n",
    "\n",
    "    def update(self, real_logits):\n",
    "        current_accuracy = tf.reduce_mean(step(real_logits))\n",
    "\n",
    "        # the augmentation probability is updated based on the dicriminator's\n",
    "        # accuracy on real images\n",
    "        accuracy_error = current_accuracy - target_accuracy\n",
    "        self.probability.assign(\n",
    "            tf.clip_by_value(\n",
    "                self.probability + accuracy_error / integration_steps, 0.0, 1.0\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f2cb39",
   "metadata": {},
   "source": [
    "### Modelos"
   ]
  },
  {
   "cell_type": "raw",
   "id": "055c015b",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "\n",
    "# Create the discriminator\n",
    "tam_ini = 64\n",
    "discriminator = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(BASE_SIZE, BASE_SIZE, 3)),\n",
    "        layers.Conv2D(tam_ini, (5, 5), strides=(2, 2), padding=\"same\",kernel_initializer='glorot_uniform'),\n",
    "       # layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(tam_ini*2, (5, 5), strides=(2, 2), padding=\"same\",kernel_initializer='glorot_uniform'),\n",
    "        layers.BatchNormalization(momentum=0.5),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(tam_ini*4, (5, 5), strides=(2, 2), padding=\"same\",kernel_initializer='glorot_uniform'),\n",
    "        layers.BatchNormalization(momentum=0.5),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(tam_ini*8, (5, 5), strides=(2, 2), padding=\"same\",kernel_initializer='glorot_uniform'),\n",
    "        layers.BatchNormalization(momentum=0.5),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Flatten(),\n",
    "        #layers.GlobalMaxPooling2D(),\n",
    "        layers.Dense(1),\n",
    "        layers.Activation('sigmoid')\n",
    "    ],\n",
    "    name=\"discriminator\",\n",
    ")\n",
    "\n",
    "discriminator.summary()\n",
    "def conv3x3(out_planes):\n",
    "    \"3x3 convolution with padding\"\n",
    "    return layers.Conv2D(out_planes, (5, 5), strides=(1, 1), padding='same', kernel_initializer='glorot_uniform')\n",
    "\n",
    "# Upsale the spatial size by a factor of 2\n",
    "def upBlock(out_planes):\n",
    "    block =  tf.keras.Sequential([\n",
    "        tf.keras.layers.UpSampling2D(size=(2, 2)),\n",
    "        conv3x3(out_planes),\n",
    "        #layers.Dropout(0.2),\n",
    "        tf.keras.layers.BatchNormalization(momentum=0.5),\n",
    "        layers.Activation(\"relu\"),\n",
    "\n",
    "    ])\n",
    "    return block\n",
    "\n",
    "def Generator():\n",
    "    tam_ini = 512\n",
    "    inputs_noise = tf.keras.layers.Input(shape=(NOISE_SIZE,))#output 100\n",
    "    x = layers.Dense(4*4*tam_ini, use_bias=False, kernel_initializer='glorot_uniform')(inputs_noise) #l x por GLU que me divide el tamaño\n",
    "    #x = layers.Dropout(0.2)(x)\n",
    "   # \n",
    "    x = layers.Reshape((4,4, tam_ini))(x)\n",
    "    x = layers.BatchNormalization(momentum=0.5)(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = upBlock(tam_ini//2)(x)\n",
    "    x = upBlock(tam_ini//4)(x)\n",
    "    x = upBlock(tam_ini//8)(x)  \n",
    "    x = upBlock(tam_ini//16)(x)\n",
    "    #x = upBlock(tam_ini//32)(x)\n",
    "    x = conv3x3(3)(x)\n",
    "    x = tf.keras.activations.tanh(x)\n",
    "    return tf.keras.Model(inputs_noise, x,  name=\"generator\")\n",
    "generator = Generator()\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ab840ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DCGAN generator\n",
    "def get_generator():\n",
    "    noise_input = keras.Input(shape=(NOISE_SIZE,))\n",
    "    x = layers.Dense(4 * 4 * width, use_bias=False)(noise_input)\n",
    "    x = layers.BatchNormalization(scale=False)(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Reshape(target_shape=(4, 4, width))(x)\n",
    "    for _ in range(depth - 1):\n",
    "        x = layers.Conv2DTranspose(\n",
    "            width, kernel_size=4, strides=2, padding=\"same\", use_bias=False,\n",
    "        )(x)\n",
    "        x = layers.BatchNormalization(scale=False)(x)\n",
    "        x = layers.ReLU()(x)\n",
    "    image_output = layers.Conv2DTranspose(\n",
    "        3, kernel_size=4, strides=2, padding=\"same\", activation=\"tanh\",\n",
    "    )(x)\n",
    "\n",
    "    return keras.Model(noise_input, image_output, name=\"generator\")\n",
    "\n",
    "\n",
    "# DCGAN discriminator\n",
    "def get_discriminator():\n",
    "    image_input = keras.Input(shape=(BASE_SIZE, BASE_SIZE, 3))\n",
    "    x = image_input\n",
    "    for _ in range(depth):\n",
    "        x = layers.Conv2D(\n",
    "            width, kernel_size=4, strides=2, padding=\"same\", use_bias=False,\n",
    "        )(x)\n",
    "        x = layers.BatchNormalization(scale=False)(x)\n",
    "        x = layers.LeakyReLU(alpha=leaky_relu_slope)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    output_score = layers.Dense(1)(x)\n",
    "    output_score =layers.Activation('sigmoid')(output_score)\n",
    "\n",
    "    return keras.Model(image_input, output_score, name=\"discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d9c9509a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load filenames from: ../data/birds/train/filenames.pickle (8855)\n",
      "Load filenames from: ../data/birds/train/filenames.pickle (8855)\n",
      "filepath ../data/birds\\captions.pickle\n",
      "Load from:  ../data/birds\\captions.pickle\n",
      "Total filenames:  11788 001.Black_footed_Albatross/Black_Footed_Albatross_0046_18.jpg\n",
      "Load filenames from: ../data/birds/test/filenames.pickle (2933)\n",
      "Load filenames from: ../data/birds/test/filenames.pickle (2933)\n",
      "filepath ../data/birds\\captions.pickle\n",
      "Load from:  ../data/birds\\captions.pickle\n",
      "Total filenames:  11788 001.Black_footed_Albatross/Black_Footed_Albatross_0046_18.jpg\n",
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_42 (InputLayer)       [(None, 128)]             0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 2048)              262144    \n",
      "                                                                 \n",
      " batch_normalization_726 (Ba  (None, 2048)             6144      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " re_lu_36 (ReLU)             (None, 2048)              0         \n",
      "                                                                 \n",
      " reshape_9 (Reshape)         (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_36 (Conv2D  (None, 8, 8, 128)        262144    \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_727 (Ba  (None, 8, 8, 128)        384       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " re_lu_37 (ReLU)             (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_37 (Conv2D  (None, 16, 16, 128)      262144    \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_728 (Ba  (None, 16, 16, 128)      384       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " re_lu_38 (ReLU)             (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_38 (Conv2D  (None, 32, 32, 128)      262144    \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_729 (Ba  (None, 32, 32, 128)      384       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " re_lu_39 (ReLU)             (None, 32, 32, 128)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_39 (Conv2D  (None, 64, 64, 3)        6147      \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,062,019\n",
      "Trainable params: 1,057,155\n",
      "Non-trainable params: 4,864\n",
      "_________________________________________________________________\n",
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_43 (InputLayer)       [(None, 64, 64, 3)]       0         \n",
      "                                                                 \n",
      " conv2d_690 (Conv2D)         (None, 32, 32, 128)       6144      \n",
      "                                                                 \n",
      " batch_normalization_730 (Ba  (None, 32, 32, 128)      384       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_32 (LeakyReLU)  (None, 32, 32, 128)       0         \n",
      "                                                                 \n",
      " conv2d_691 (Conv2D)         (None, 16, 16, 128)       262144    \n",
      "                                                                 \n",
      " batch_normalization_731 (Ba  (None, 16, 16, 128)      384       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_33 (LeakyReLU)  (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " conv2d_692 (Conv2D)         (None, 8, 8, 128)         262144    \n",
      "                                                                 \n",
      " batch_normalization_732 (Ba  (None, 8, 8, 128)        384       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_34 (LeakyReLU)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv2d_693 (Conv2D)         (None, 4, 4, 128)         262144    \n",
      "                                                                 \n",
      " batch_normalization_733 (Ba  (None, 4, 4, 128)        384       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_35 (LeakyReLU)  (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 1)                 2049      \n",
      "                                                                 \n",
      " activation_658 (Activation)  (None, 1)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 796,161\n",
      "Trainable params: 795,137\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "epochs = 300\n",
    "batch_size = 64\n",
    "learning_rate = 2e-4\n",
    "beta_1 = 0.5\n",
    "beta_2 = 0.9\n",
    "num_train_gen = 1 #por cada batch cuantas veces entrenamos el generador\n",
    "num_train_disc = 1\n",
    "cuantas_imgs_vis = 3 #imagenes que vamos viendo al entrenar al final de cada epoca\n",
    "kl_divergence_loss_coeff = 1\n",
    "\n",
    "split = \"train\"\n",
    "prepare_caption_train = PREPARE_CAPTIONS(data_dir, split)\n",
    "filenames_train, captions_train = prepare_caption.load_text_data(data_dir, split)\n",
    "prepare_image_train = PREPARE_IMAGE(data_dir, split, captions_train, filenames_train, BASE_SIZE)\n",
    "\n",
    "dataset_train = tf.data.Dataset.range(len(filenames_train)) \n",
    "dataset_train = dataset_train.shuffle(buffer_size=len(filenames_train)) # comment this line if you don't want to shuffle data\n",
    "dataset_train = dataset_train.batch(batch_size=batch_size, drop_remainder=True)\n",
    "dataset_train = dataset_train.repeat(epochs)\n",
    "\n",
    "split = \"test\"\n",
    "prepare_caption_test = PREPARE_CAPTIONS(data_dir, split)\n",
    "filenames_test, captions_test = prepare_caption.load_text_data(data_dir, split)\n",
    "prepare_image_test = PREPARE_IMAGE(data_dir, split, captions_test, filenames_test, BASE_SIZE)\n",
    "\n",
    "dataset_test = tf.data.Dataset.range(len(filenames_test)) \n",
    "dataset_test = dataset_test.shuffle(buffer_size=len(filenames_test)) # comment this line if you don't want to shuffle data\n",
    "dataset_test = dataset_test.batch(batch_size=batch_size, drop_remainder=True)\n",
    "#dataset_test = dataset_test.repeat(epochs)\n",
    "\n",
    "augmenter = AdaptiveAugmenter()\n",
    "generator = get_generator()\n",
    "ema_generator = keras.models.clone_model(generator)\n",
    "discriminator = get_discriminator()\n",
    "generator.summary()\n",
    "discriminator.summary()\n",
    "kid = KID()\n",
    "#discriminator = Discriminador()\n",
    "#generator = Generator()\n",
    "#model_clip = TFCLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "#tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "d_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2)\n",
    "g_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2)\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy()#from_logits=True)\n",
    "\n",
    "def train_step(real_images):\n",
    "    # Sample random points in the latent space\n",
    "    loss_disc = 0\n",
    "    real_images = tf.convert_to_tensor(real_images)\n",
    "    real_images = augmenter(real_images)\n",
    "\n",
    "    for _ in range(num_train_disc):\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, NOISE_SIZE))\n",
    "        # Decode them to fake images\n",
    "        generated_images = generator(random_latent_vectors)\n",
    "        # gradient is calculated through the image augmentation\n",
    "        generated_images = augmenter(generated_images)\n",
    "        labels = tf.zeros((batch_size, 1))\n",
    "        # Add random noise to the labels - important trick!\n",
    "        labels += 0.2 * tf.random.uniform(labels.shape)\n",
    "    \n",
    "        # Train the discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = discriminator(generated_images)\n",
    "            #predictions_real = discriminator(real_images)\n",
    "            d_loss = loss_fn(labels, predictions)\n",
    "            #gp = gradient_penalty(batch_size, real_images, generated_images)\n",
    "            #d_loss = discriminator_loss(predictions_real, predictions_fake)+10*gp\n",
    "            loss_disc +=d_loss\n",
    "        grads = tape.gradient(d_loss, discriminator.trainable_weights)\n",
    "        d_optimizer.apply_gradients(zip(grads, discriminator.trainable_weights))\n",
    "        \n",
    "        labels = tf.ones((batch_size, 1))\n",
    "        # Add random noise to the labels - important trick!\n",
    "        labels -= 0.2 * tf.random.uniform(labels.shape)\n",
    "        # Train the discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = discriminator(real_images)\n",
    "            d_loss = loss_fn(labels, predictions)\n",
    "            loss_disc +=d_loss\n",
    "        grads = tape.gradient(d_loss, discriminator.trainable_weights)\n",
    "        d_optimizer.apply_gradients(zip(grads, discriminator.trainable_weights))\n",
    "    augmenter.update(predictions)\n",
    "    # Sample random points in the latent space\n",
    "    loss_gen = 0\n",
    "    misleading_labels = tf.ones((2*batch_size, 1))\n",
    "\n",
    "    for _ in range(num_train_gen):\n",
    "        random_latent_vectors = tf.random.normal(shape=(2*batch_size, NOISE_SIZE))\n",
    "        # Assemble labels that say \"all real images\"\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            generated_images = generator(random_latent_vectors)\n",
    "            generated_images = augmenter(generated_images)\n",
    "            predictions = discriminator(generated_images)\n",
    "            g_loss = loss_fn(misleading_labels, predictions)\n",
    "            #g_loss = generator_loss(predictions)\n",
    "            loss_gen += g_loss \n",
    "        grads = tape.gradient(g_loss, generator.trainable_weights)\n",
    "        g_optimizer.apply_gradients(zip(grads, generator.trainable_weights))\n",
    "        \n",
    "        \n",
    "        \n",
    "    # track the exponential moving average of the generator's weights to decrease\n",
    "    # variance in the generation quality\n",
    "    for weight, ema_weight in zip(generator.weights, ema_generator.weights):\n",
    "        ema_weight.assign(ema * ema_weight + (1 - ema) * weight)\n",
    "        \n",
    "    return loss_disc, loss_gen\n",
    "\n",
    "\n",
    "\n",
    "def generate(batch_size, training):\n",
    "    latent_samples = tf.random.normal(shape=(batch_size, NOISE_SIZE))\n",
    "    # use ema_generator during inference\n",
    "    if training:\n",
    "        generated_images = generator(latent_samples, training)\n",
    "    else:\n",
    "        generated_images = ema_generator(latent_samples, training)\n",
    "    return generated_images\n",
    "\n",
    "def test_step(real_images):\n",
    "    # KID is not measured during the training phase for computational efficiency\n",
    "\n",
    "    real_images = tf.convert_to_tensor(real_images)\n",
    "\n",
    "    generated_images = generate(batch_size, training=False)\n",
    "\n",
    "    kid.update_state(real_images, generated_images)\n",
    "\n",
    "    # only KID is measured during the evaluation phase for computational efficiency\n",
    "    return {kid.name: kid.result()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7353e259",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoca = 0\n",
    "num_batch = 0\n",
    "when_end_epoch =int(len(filenames_train)/batch_size+1)\n",
    "print(\"EPOCA:\", epoca)\n",
    "\n",
    "disc_loss_tracker_batch = []\n",
    "gen_loss_tracker_batch = []\n",
    "#kl_loss_tracker_batch = []\n",
    "disc_loss_tracker_epoch = []\n",
    "gen_loss_tracker_epoch = []\n",
    "#kl_loss_tracker_epoch = []\n",
    "kid_accuracy_epoch = []\n",
    "for batch in tqdm(dataset_train):\n",
    "    if num_batch!=0 and num_batch%when_end_epoch==0:\n",
    "        # test\n",
    "        for batch_test in tqdm(dataset_test):\n",
    "            imgs_list_test = []\n",
    "            #caps_list_test = []\n",
    "            for ind in batch_test:\n",
    "                imgs, caps, cls_id, key, wrong_caps, wrong_cls_id = prepare_image_test.__getitem__(ind)\n",
    "                imgs_list_test.append(imgs)\n",
    "                #caps_list_test.append(caps)\n",
    "            kid_accuracy_epoch.append(test_step(imgs_list_test)[\"kid\"].numpy())\n",
    "        print(\"KID en validation:\",np.mean(kid_accuracy_epoch))\n",
    "        # random_latent_vectors = tf.random.uniform(shape=(cuantas_imgs_vis, NOISE_SIZE), minval=-1, maxval=1)\n",
    "        random_latent_vectors = tf.random.normal(shape=(cuantas_imgs_vis, NOISE_SIZE), mean=0.0, stddev=1.0, dtype=tf.dtypes.float32)\n",
    "        fake_images = generator([random_latent_vectors])\n",
    "        for title, img in zip(caps_list[:cuantas_imgs_vis], fake_images):\n",
    "            img = (img+1)/2\n",
    "            plt.figure(figsize=(3,3))\n",
    "            plt.imshow(img)\n",
    "            plt.axis(\"off\")\n",
    "            #plt.title(title)\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "        disc_loss_tracker_epoch.append(np.mean([disc_loss_tracker_batch]))\n",
    "        gen_loss_tracker_epoch.append(np.mean([gen_loss_tracker_batch]))\n",
    "        #kl_loss_tracker_epoch.append(np.mean([kl_loss_tracker_batch]))\n",
    "\n",
    "        epoca+=1\n",
    "        print(\"EPOCA:\", epoca)\n",
    "        disc_loss_tracker_batch = []\n",
    "        gen_loss_tracker_batch = []\n",
    "        #kl_loss_tracker_batch = []\n",
    "\n",
    "    # Cargo el batch y lo preparo ordenandolo en funcion de mas o menos palabras en el cap\n",
    "    # imgs_list, caps_list, cap_len_list, cls_id_list, key_list, wrong_caps_list, wrong_cap_len_list, wrong_cls_id_list = [], [], [], [], [], [], [], [] \n",
    "    imgs_list, caps_list, cls_id_list, key_list, wrong_caps_list, wrong_cls_id_list = [], [], [], [], [], []\n",
    "\n",
    "    for ind in batch:\n",
    "        imgs, caps, cls_id, key, wrong_caps, wrong_cls_id = prepare_image_train.__getitem__(ind)\n",
    "        imgs_list.append(imgs)\n",
    "        caps_list.append(caps)\n",
    "\n",
    "    loss_disc, loss_gen = train_step(imgs_list)\n",
    "    disc_loss_tracker_batch.append(loss_disc/num_train_disc)\n",
    "    gen_loss_tracker_batch.append(loss_gen/num_train_gen)\n",
    "    if num_batch%(when_end_epoch//10)==0:\n",
    "        print(\"Loss DISC:\", np.mean([disc_loss_tracker_batch]), \"Loss GEN:\", np.mean([gen_loss_tracker_batch]))\n",
    "        \n",
    "    num_batch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39003b8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e894b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
