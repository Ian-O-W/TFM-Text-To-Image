{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85f70998",
   "metadata": {},
   "source": [
    "+ 8855 imágenes de train \n",
    "+ 2933 imágenes de test\n",
    "+ 10 captions por imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b783d7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.random as random\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "from collections import defaultdict # diccionario que al asignar un nuevo valor crea la clave\n",
    "if sys.version_info[0] == 2:\n",
    "    import cPickle as pickle\n",
    "else:\n",
    "    import pickle\n",
    "    \n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer # A tokenizer that splits a string using a regular expression, which matches either the tokens or the separators between tokens.\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from transformers import CLIPTokenizer, TFCLIPTextModel\n",
    "from tensorflow.keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07790e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-23 14:10:27.942142: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2022-03-23 14:10:27.942198: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: ian\n",
      "2022-03-23 14:10:27.942202: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: ian\n",
      "2022-03-23 14:10:27.942280: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 510.54.0\n",
      "2022-03-23 14:10:27.942297: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 510.54.0\n",
      "2022-03-23 14:10:27.942300: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 510.54.0\n"
     ]
    }
   ],
   "source": [
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9915c7",
   "metadata": {},
   "source": [
    "# PARAMS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5179fee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../birds\"\n",
    "EMBEDDINGS_NUM = 10 # OLD número de captions por imagen\n",
    "WORDS_NUM = 18 # número maximo de palabras por caption. Si es menos se hace padding, si es mas se trunca\n",
    "BRANCH_NUM = 0  # número de veces que cargo cada imagen. Si es 0 cargamos solo la imagen del tamaño dado\n",
    "BASE_SIZE = 64 # Tamaño mínimo de las imagenes. Cada imagen que cargaremos tendrá el doble de tamaño que la anterior. la última el tamaño original \n",
    "NOISE_SIZE = 128 # power of 2\n",
    "CONDITION_DIM = 100\n",
    "OUT_EMB = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6582a7",
   "metadata": {},
   "source": [
    "Guía de funcioens de carga\n",
    "https://github.com/mrlibw/ControlGAN/blob/master/code/datasets.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe414e05",
   "metadata": {},
   "source": [
    "# LOAD AND PREPARE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f9e8018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PREPARE_CAPTIONS():\n",
    "    \"\"\"\n",
    "    CLASE PARA PREPARAR EL PICKLE DE CAPTIONS. \n",
    "    PASAMOS A INDÍCES LOS CAPTIONS (SEGÚN VOCABULARIO CREADO)\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, split='train'):\n",
    "        self.EMBEDDINGS_NUM = EMBEDDINGS_NUM\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "    def load_filenames(self, data_dir, filenames):\n",
    "        \"\"\"\n",
    "        Cargamos los pickles de los filenames. Contienen los names de la imagen/caption.\n",
    "        \"\"\"\n",
    "\n",
    "        filepath = '%s/%s/filenames.pickle' % (data_dir, split)\n",
    "        if os.path.isfile(filepath):\n",
    "            with open(filepath, 'rb') as f:\n",
    "                filenames = pickle.load(f)\n",
    "            print('Load filenames from: %s (%d)' % (filepath, len(filenames)))\n",
    "        else:\n",
    "            filenames = []\n",
    "        return filenames\n",
    "\n",
    "    def load_captions(self, data_dir, filenames):\n",
    "        all_captions = []\n",
    "        for i in range(len(filenames)):\n",
    "            cap_path = '%s/text/%s.txt' % (data_dir, filenames[i])\n",
    "            with open(cap_path, \"r\") as f:\n",
    "                # Leemos cada línea y lo metemos en una lista \n",
    "                captions = f.read().split('\\n')\n",
    "                cnt = 0\n",
    "                for cap in captions:\n",
    "                    if len(cap) == 0:\n",
    "                        continue\n",
    "                    cap = cap.replace(\"\\ufffd\\ufffd\", \" \")\n",
    "                    # Nos quedamos solo con secuencias alfanuméricas como tokens, borramos el resto\n",
    "#                    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#                    tokens = tokenizer.tokenize(cap.lower())\n",
    "#                    if len(tokens) == 0:\n",
    "#                        print('cap', cap)\n",
    "#                        continue\n",
    "#                    tokens_new = []\n",
    "#                    for t in tokens:\n",
    "#                        t = t.encode('ascii', 'ignore').decode('ascii')\n",
    "#                        if len(t) > 0:\n",
    "#                            tokens_new.append(t)\n",
    "#                    all_captions.append(tokens_new)\n",
    "                    all_captions.append(cap)\n",
    "#                    cnt += 1\n",
    "#                    if cnt == self.EMBEDDINGS_NUM:\n",
    "#                        break\n",
    "#                if cnt < self.EMBEDDINGS_NUM:\n",
    "#                    print('ERROR: the captions for %s less than %d' % (filenames[i], cnt))\n",
    "        return all_captions\n",
    "\n",
    "    def build_dictionary(self, train_captions, test_captions):\n",
    "        \"\"\"\n",
    "        Creo diccionarios. \n",
    "        Vocabulario.\n",
    "        Captions como indices.\n",
    "        Indíces a words\n",
    "        Words a índices\n",
    "        Número de palabras.\n",
    "        \"\"\"\n",
    "\n",
    "        # Creo diccionario de palabras existentes: número de aparaciones \n",
    "        word_counts = defaultdict(float)\n",
    "        captions = train_captions + test_captions\n",
    "        for sent in captions:\n",
    "            for word in sent:\n",
    "                word_counts[word] += 1\n",
    "\n",
    "        # Creo vocabulario\n",
    "        vocab = [w for w in word_counts if word_counts[w] >= 0]\n",
    "        # Añado al vocabulario \"end\"\n",
    "        # Creo los intercambios de indices-palabras y viceversa\n",
    "        ixtoword = {}\n",
    "        ixtoword[0] = '<end>'\n",
    "        wordtoix = {}\n",
    "        wordtoix['<end>'] = 0\n",
    "        ix = 1\n",
    "        for w in vocab:\n",
    "            wordtoix[w] = ix\n",
    "            ixtoword[ix] = w\n",
    "            ix += 1\n",
    "\n",
    "        # Train: pasar los captions a indices\n",
    "        train_captions_new = []\n",
    "        for t in train_captions:\n",
    "            rev = []\n",
    "            for w in t:\n",
    "                if w in wordtoix:\n",
    "                    rev.append(wordtoix[w])\n",
    "            train_captions_new.append(rev)\n",
    "\n",
    "        # Test: pasar los captions a indices\n",
    "        test_captions_new = []\n",
    "        for t in test_captions:\n",
    "            rev = []\n",
    "            for w in t:\n",
    "                if w in wordtoix:\n",
    "                    rev.append(wordtoix[w])\n",
    "            test_captions_new.append(rev)\n",
    "\n",
    "        return [train_captions_new, test_captions_new, ixtoword, wordtoix, len(ixtoword)]\n",
    "\n",
    "    def load_text_data(self, data_dir, split):\n",
    "        \"\"\"\n",
    "        Funcion para leer los pickles segun splits\n",
    "        Return: \n",
    "            + filenames: \n",
    "            + captions: \n",
    "            + ixtoword: \n",
    "            + wordtoix: \n",
    "            + n_words:  número de palabras\n",
    "        \"\"\"\n",
    "        # Cargamos los pickles de los splits:\n",
    "        filepath = os.path.join(data_dir, 'captions.pickle')\n",
    "        train_names = self.load_filenames(data_dir, 'train')\n",
    "        test_names = self.load_filenames(data_dir, 'test')\n",
    "\n",
    "        # Si no existe creamos el captions.pickle con los captions como indices y los diccionarios \n",
    "        # para pasar al vocabulario\n",
    "        if not os.path.isfile(filepath):\n",
    "            # Cargamos los descriptores\n",
    "            train_captions = self.load_captions(data_dir, train_names)\n",
    "            test_captions = self.load_captions(data_dir, test_names)\n",
    "            # Diccionario de vocabulario (y de índice a palabras y viceversa)\n",
    "            # y paso los captions a índices:\n",
    "            #train_captions, test_captions, ixtoword, wordtoix, n_words = self.build_dictionary(train_captions, test_captions)\n",
    "            #Guardo todo esto en pickles\n",
    "            with open(filepath, 'wb') as f:\n",
    "                pickle.dump([train_captions, test_captions], f, protocol=2)#  ,ixtoword, wordtoix], f, protocol=2)\n",
    "                print('Save to: ', filepath)\n",
    "        # Si existe ccaptions.picke lo cargamos \n",
    "        else:\n",
    "            with open(filepath, 'rb') as f:\n",
    "                print(\"filepath\", filepath)\n",
    "                x = pickle.load(f)\n",
    "                train_captions, test_captions = x[0], x[1]\n",
    "                #ixtoword, wordtoix = x[2], x[3]\n",
    "                del x\n",
    "                #n_words = len(ixtoword)\n",
    "                print('Load from: ', filepath)\n",
    "        if split == 'train':\n",
    "            # a list of list: each list contains\n",
    "            # the indices of words in a sentence\n",
    "            captions = train_captions\n",
    "            filenames = train_names\n",
    "        else:  # split=='test'\n",
    "            captions = test_captions\n",
    "            filenames = test_names\n",
    "        return filenames, captions#, ixtoword, wordtoix, n_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7c655f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PREPARE_IMAGE():\n",
    "    \n",
    "    def __init__(self, data_dir, split, captions, filenames, BASE_SIZE):\n",
    "        self.data_dir = data_dir\n",
    "        self.imsize = []\n",
    "        self.BASE_SIZE = BASE_SIZE\n",
    "        self.captions = captions\n",
    "        self.filenames = filenames \n",
    "        # tamaños a los que escalo\n",
    "        if BRANCH_NUM>0:\n",
    "            for i in range(BRANCH_NUM):\n",
    "                self.imsize.append(self.BASE_SIZE)\n",
    "                self.BASE_SIZE = self.BASE_SIZE * 2\n",
    "        else:\n",
    "            self.imsize.append(self.BASE_SIZE)\n",
    "        self.bbox = self.load_bbox()        \n",
    "        self.class_id = self.load_class_id(data_dir, split, len(self.filenames))\n",
    "    def get_imgs(self, img_path, imsize, bbox=None):\n",
    "        \"\"\"\n",
    "        Función de carga de imagen.\n",
    "        Realizo crop de la bbox si existe\n",
    "        \"\"\"\n",
    "        \n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        width, height = img.size\n",
    "        # Realizo crop de la bbox si existe\n",
    "        if bbox is not None:\n",
    "            r = int(np.maximum(bbox[2], bbox[3]) * 0.75)\n",
    "            center_x = int((2 * bbox[0] + bbox[2]) / 2)\n",
    "            center_y = int((2 * bbox[1] + bbox[3]) / 2)\n",
    "            y1 = np.maximum(0, center_y - r)\n",
    "            y2 = np.minimum(height, center_y + r)\n",
    "            x1 = np.maximum(0, center_x - r)\n",
    "            x2 = np.minimum(width, center_x + r)\n",
    "            img = img.crop([x1, y1, x2, y2])\n",
    "        #aumentado con flip\n",
    "        if np.random.rand()<0.5:\n",
    "            img = np.fliplr(img)\n",
    "        # Escalo la imagen a distintos tamaños \n",
    "        # La última de las veces NO\n",
    "        if BRANCH_NUM>0:\n",
    "            ret = []\n",
    "            for i in range(BRANCH_NUM):\n",
    "                #if i < (BRANCH_NUM - 1):\n",
    "                    # re_img = transforms.Scale(imsize[i])(img)\n",
    "                re_img = tf.image.resize(img, size=(imsize[i], imsize[i]))\n",
    "                #else:\n",
    "                #    re_img = img\n",
    "                ret.append((tf.cast(re_img, tf.float32) - 127.5) / 127.5)\n",
    "        else:\n",
    "            ret = tf.image.resize(img, size=(imsize[0], imsize[0]))\n",
    "            ret = (tf.cast(ret, tf.float32) - 127.5) / 127.5 #/255\n",
    "        return ret  \n",
    "    \n",
    "    def load_class_id(self, data_dir, split, total_num):\n",
    "        \"\"\"\n",
    "        Cargamos la información de a que clase pertenece (por id)\n",
    "        \"\"\"\n",
    "        split_dir = os.path.join(data_dir, split)\n",
    "        if os.path.isfile(split_dir + '/class_info.pickle'):\n",
    "            with open(split_dir + '/class_info.pickle', 'rb') as f:\n",
    "                class_id = pickle.load(f, encoding='latin1')\n",
    "        else:\n",
    "            class_id = np.arange(total_num)\n",
    "        return tf.convert_to_tensor(class_id)\n",
    "    \n",
    "    \n",
    "    def load_bbox(self):\n",
    "        \"\"\"\n",
    "        Leemos las BBox\n",
    "        \"\"\"\n",
    "        data_dir = self.data_dir\n",
    "        bbox_path = os.path.join(data_dir, 'bounding_boxes.txt')\n",
    "        df_bounding_boxes = pd.read_csv(bbox_path,\n",
    "                                        delim_whitespace=True,\n",
    "                                        header=None).astype(int)\n",
    "\n",
    "        filepath = os.path.join(data_dir, 'images.txt')\n",
    "        df_filenames = pd.read_csv(filepath, delim_whitespace=True, header=None)\n",
    "        filenames = df_filenames[1].tolist() #paths de todas las imágenes\n",
    "        print('Total filenames: ', len(filenames), filenames[0])\n",
    "        \n",
    "        # Paths BBOX\n",
    "        filename_bbox = {img_file[:-4]: [] for img_file in filenames}\n",
    "        numImgs = len(filenames)\n",
    "        \n",
    "        # Guardo en un diccionario el path y la bbox\n",
    "        for i in range(0, numImgs):\n",
    "            bbox = df_bounding_boxes.iloc[i][1:].tolist() #la bbox i\n",
    "            key = filenames[i][:-4]\n",
    "            filename_bbox[key] = bbox\n",
    "        return filename_bbox\n",
    "    \n",
    "    \n",
    "    def get_caption(self, sent_ix):\n",
    "        \"\"\"\n",
    "        sent_ix: numero random del 0 al embeddings num (número de frases por imagen)\n",
    "        Devuelve un vector de índices del número de palabras fijados (rellenado con 0 si es necesario) \n",
    "        \"\"\"\n",
    "        \n",
    "        # Selecciono una frase al azar\n",
    "        sent_caption = np.asarray(self.captions[sent_ix]).astype('int64')\n",
    "        if (sent_caption == 0).sum() > 0:\n",
    "            print('ERROR: do not need END (0) token', sent_caption)\n",
    "            \n",
    "        # Creo una vector del tamaño fijado de las frases. Relleno con 0 si hace falta\n",
    "        # si hay más palabras del número fijado selecciono al azar de la frase ese número de palabras\n",
    "        num_words = len(sent_caption)\n",
    "        x = np.zeros((WORDS_NUM, 1), dtype='int64')\n",
    "        x_len = num_words\n",
    "        if num_words <= WORDS_NUM:\n",
    "            x[:num_words, 0] = sent_caption\n",
    "        else:\n",
    "            ix = list(np.arange(num_words))  # 1, 2, 3,..., maxNum\n",
    "            np.random.shuffle(ix)\n",
    "            ix = ix[:WORDS_NUM]\n",
    "            ix = np.sort(ix)\n",
    "            x[:, 0] = sent_caption[ix]\n",
    "            x_len = WORDS_NUM\n",
    "        x = np.squeeze(x)\n",
    "\n",
    "        return tf.convert_to_tensor(x), tf.convert_to_tensor(x_len)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Dado un indice obtenemos:\n",
    "        + Imagen BRANCH_NUM veces. A distintos tamaños y recortada\n",
    "        + Un caption bueno (junto a su clase (tipo de pájaro))\n",
    "        + Un caption malo (junto a su clase (tipo de pájaro))\n",
    "        + Longitud del caption (fijada por WORDS_NUM)\n",
    "        \"\"\"\n",
    "        key = self.filenames[index]\n",
    "        #cls_id = self.class_id[index]\n",
    "\n",
    "        if self.bbox is not None:\n",
    "            bbox = self.bbox[key]\n",
    "        #    data_dir = '%s/CUB_200_2011' % self.data_dir\n",
    "        else:\n",
    "            bbox = None\n",
    "        #    data_dir = self.data_dir\n",
    "        # Cargo la imagen BRANCH_NUM veces. A distintos tamaños y recortada\n",
    "        img_name = '%s/images/%s.jpg' % (data_dir, key)\n",
    "        imgs = self.get_imgs(img_name, self.imsize, bbox)\n",
    "        # Selecciono un caption al azar y lo cargo\n",
    "        sent_ix = random.randint(0, EMBEDDINGS_NUM)\n",
    "        new_sent_ix = index * EMBEDDINGS_NUM + sent_ix\n",
    "        #caps, cap_len = self.get_caption(new_sent_ix)\n",
    "        caps = self.captions[new_sent_ix]\n",
    "        \n",
    "        # Selecciono al azar un caption ERRONEO\n",
    "        #wrong_idx = random.randint(0, len(self.filenames))\n",
    "        #wrong_new_sent_ix = wrong_idx * EMBEDDINGS_NUM + sent_ix\n",
    "        #wrong_caps, wrong_cap_len = self.get_caption(wrong_new_sent_ix)\n",
    "        #wrong_caps = self.captions[wrong_new_sent_ix]\n",
    "        #wrong_cls_id = self.class_id[wrong_idx]\n",
    "\n",
    "        #return imgs, caps, cap_len, cls_id, key, wrong_caps, wrong_cap_len, wrong_cls_id\n",
    "       # return imgs, caps, cls_id, key, wrong_caps, wrong_cls_id\n",
    "        return imgs, caps, None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d110c71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load filenames from: ../birds/train/filenames.pickle (8855)\n",
      "Load filenames from: ../birds/train/filenames.pickle (8855)\n",
      "filepath ../birds/captions.pickle\n",
      "Load from:  ../birds/captions.pickle\n",
      "Load filenames from: ../birds/test/filenames.pickle (2933)\n",
      "Load filenames from: ../birds/test/filenames.pickle (2933)\n",
      "filepath ../birds/captions.pickle\n",
      "Load from:  ../birds/captions.pickle\n"
     ]
    }
   ],
   "source": [
    "split = \"train\"\n",
    "prepare_caption1 = PREPARE_CAPTIONS(data_dir, split)\n",
    "#filenames, captions, ixtoword, wordtoix, n_words = prepare_caption.load_text_data(data_dir, split)\n",
    "filenames1, captions1 = prepare_caption1.load_text_data(data_dir, split)\n",
    "\n",
    "split = \"test\"\n",
    "prepare_caption2 = PREPARE_CAPTIONS(data_dir, split)\n",
    "#filenames, captions, ixtoword, wordtoix, n_words = prepare_caption.load_text_data(data_dir, split)\n",
    "filenames2, captions2 = prepare_caption2.load_text_data(data_dir, split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f8f4cf72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11788"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filenames1 +filenames2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70f85ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load filenames from: ../birds/train/filenames.pickle (8855)\n",
      "Load filenames from: ../birds/train/filenames.pickle (8855)\n",
      "filepath ../birds/captions.pickle\n",
      "Load from:  ../birds/captions.pickle\n"
     ]
    }
   ],
   "source": [
    "split = \"train\"\n",
    "prepare_caption = PREPARE_CAPTIONS(data_dir, split)\n",
    "#filenames, captions, ixtoword, wordtoix, n_words = prepare_caption.load_text_data(data_dir, split)\n",
    "filenames, captions = prepare_caption.load_text_data(data_dir, split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05925451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total filenames:  11788 001.Black_footed_Albatross/Black_Footed_Albatross_0046_18.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-22 18:35:26.989499: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-22 18:35:27.881379: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10200 MB memory:  -> device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:06:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "prepare_image = PREPARE_IMAGE(data_dir, split, captions, filenames, BASE_SIZE)\n",
    "#imgs, caps, cap_len, cls_id, key, wrong_caps, wrong_cap_len, wrong_cls_id = prepare_image.__getitem__(5)\n",
    "imgs, caps, cls_id, key, wrong_caps, wrong_cls_id = prepare_image.__getitem__(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a71ebca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7fc231afd0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(imgs[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473786ac",
   "metadata": {},
   "source": [
    "# MODELOS"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8d6948be",
   "metadata": {},
   "source": [
    "# Revisar posibilidad de no meter capas densas y simplemente hacer la lineal de la primera mitad de los elementos y sumar la sigmoid de la segunda mitad de los mismos\n",
    "class GatedLinearUnit(layers.Layer):\n",
    "    def __init__(self, units=200):\n",
    "        super(GatedLinearUnit, self).__init__()\n",
    "        #self.linear = layers.Dense(units)\n",
    "        #self.sigmoid = layers.Dense(units, activation=\"sigmoid\")\n",
    "    def call(self, inputs):\n",
    "        nc = inputs.shape[1]\n",
    "        assert nc % 2 == 0, 'channels dont divide 2!'\n",
    "        nc = int(nc/2)      \n",
    "        #return self.linear(inputs) * self.sigmoid(inputs)\n",
    "        return tf.math.multiply(inputs[:,:nc], tf.keras.activations.sigmoid(inputs[:, nc:]))\n",
    "\n",
    "class GatedLinearUnit2D(layers.Layer):\n",
    "    def __init__(self, units=200):\n",
    "        super(GatedLinearUnit2D, self).__init__()\n",
    "        #self.linear = layers.Dense(units)\n",
    "        #self.sigmoid = layers.Dense(units, activation=\"sigmoid\")\n",
    "    def call(self, inputs):\n",
    "        nc = inputs.shape[-1]\n",
    "        assert nc % 2 == 0, 'channels dont divide 2!'\n",
    "        nc = int(nc/2)      \n",
    "        #return self.linear(inputs) * self.sigmoid(inputs)\n",
    "        return tf.math.multiply(inputs[:,:,:,:nc], tf.keras.activations.sigmoid(inputs[:,:,:,nc:]))    \n",
    "\n",
    "def cond_aug():\n",
    "    inputs_emb = tf.keras.layers.Input(shape=(OUT_EMB,))\n",
    "    x = layers.Dense(CONDITION_DIM*4, use_bias=True)(inputs_emb)\n",
    "    x = GatedLinearUnit()(x)\n",
    "    mu = x[:, :CONDITION_DIM]\n",
    "    logvar = x[:, CONDITION_DIM:]\n",
    "    std = tf.math.exp(tf.math.multiply(logvar,0.5))\n",
    "    c_code = tf.random.normal([std.shape[1]], mean=0.0, stddev=1.0, dtype=tf.dtypes.float32)\n",
    "    c_code = tf.math.add(tf.math.multiply(std, c_code), mu)\n",
    "    return tf.keras.Model(inputs_emb, [c_code, mu, logvar], name=\"cond_aug\")\n",
    "\n",
    "projection_dims = 128\n",
    "def upsample(filters, size, apply_dropout=False,last = False):\n",
    "    initializer = tf.random_normal_initializer(0., 0.05)\n",
    "    result = tf.keras.Sequential()\n",
    "    if last:\n",
    "        result.add(tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
    "                                        padding='same',\n",
    "                                        kernel_initializer=initializer,\n",
    "                                        use_bias=False, activation='sigmoid'))\n",
    "    else:\n",
    "        result.add(\n",
    "        tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
    "                                        padding='same',\n",
    "                                        kernel_initializer=initializer,\n",
    "                                        use_bias=False))\n",
    "        result.add(tf.keras.layers.BatchNormalization())\n",
    "        if apply_dropout:\n",
    "            result.add(tf.keras.layers.Dropout(0.1))\n",
    "        result.add(tf.keras.layers.ReLU())\n",
    "    return result\n",
    "\n",
    "def Generator():\n",
    "    tam_ini = 256\n",
    "    ca_net = cond_aug()\n",
    "    \n",
    "    initializer = tf.random_normal_initializer(0., 0.05)\n",
    "    inputs_noise = tf.keras.layers.Input(shape=(NOISE_SIZE,))#output 100\n",
    "    inputs_emb = tf.keras.layers.Input(shape=(OUT_EMB,))\n",
    "    emb, mu, logvar = ca_net(inputs_emb)\n",
    "\n",
    "    #if projection_dims!=0:\n",
    "    #    emb = layers.Dense(projection_dims, use_bias=False)(inputs_emb) #output 128\n",
    "   # else:\n",
    "    #    emb = inputs_emb\n",
    "    x = tf.concat([inputs_noise, emb], 1)#output 228\n",
    "    # x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(4*4*tam_ini, use_bias=False)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = layers.Reshape((4,4, tam_ini))(x)\n",
    "    x = upsample(tam_ini, 3, apply_dropout=False)(x)\n",
    "    x = upsample(tam_ini//2, 3, apply_dropout=False)(x)\n",
    "    x = upsample(tam_ini//4, 3, apply_dropout=False)(x)\n",
    "    x = upsample(tam_ini//8, 3)(x)  \n",
    "    #x = upsample(tam_ini//16, 4)(x) \n",
    "    x = upsample(3, 3,last = True)(x)\n",
    "    return tf.keras.Model([inputs_noise, inputs_emb], [x, mu, logvar], name=\"generator\")\n",
    "\n",
    "def Discriminador():\n",
    "    tam_ini = 32\n",
    "    image_input = tf.keras.Input(shape=(BASE_SIZE, BASE_SIZE, 3))\n",
    "    inputs_emb = tf.keras.layers.Input(shape=(OUT_EMB,))\n",
    "    if projection_dims!=0:\n",
    "        emb = layers.Dense(projection_dims, use_bias=False)(inputs_emb) \n",
    "    else:\n",
    "        emb = inputs_emb \n",
    "        \n",
    "    x = layers.Conv2D(tam_ini, (4, 4), strides=(2, 2), padding='same')(image_input)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    #x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = layers.Conv2D(tam_ini*2, (4, 4), strides=(2, 2), padding='same')(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    #x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = layers.Conv2D(tam_ini*4, (4, 4), strides=(2, 2), padding='same')(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    #x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    x = layers.Conv2D(tam_ini*8, (4, 4), strides=(2, 2), padding='same')(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    #x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    # print(\"X shape:\",x.shape)\n",
    "    # x = layers.Conv2D(tam_ini*16, (4, 4), strides=(2, 2), padding='same')(x)\n",
    "    # x = layers.LeakyReLU()(x)\n",
    "    # x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "\n",
    "    #x = layers.Dropout(0.3)(x)\n",
    "    #x = layers.Conv2D(tam_ini*32, (4, 4), strides=(2, 2), padding='same')(x)\n",
    "    #x = layers.LeakyReLU()(x)\n",
    "    #x = layers.Dropout(0.3)(x)\n",
    "    # print(\"labels_input shape:\", labels_input.shape)\n",
    "    emb = layers.RepeatVector(16)(emb)\n",
    "    # print(\"After, reshape:\",emb)\n",
    "    emb = layers.Reshape((4,4, emb.shape[-1]))(emb)\n",
    "\n",
    "    x = tf.concat([x, emb], 3)\n",
    "    # print(\"After concat reshape:\",x.shape)\n",
    "    x = layers.Conv2D(1, (1, 1), strides=(1,1), padding='same')(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "    #print(\"sh:\",x.shape)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    #x = layers.Dense(100)(x)\n",
    "\n",
    "    output_score = layers.Dense(1,activation=\"sigmoid\")(x)\n",
    "\n",
    "    return tf.keras.Model([image_input, inputs_emb], output_score, name=\"discriminator\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c1fe6918",
   "metadata": {},
   "source": [
    "def conv3x3(out_planes):\n",
    "    \"3x3 convolution with padding\"\n",
    "    return layers.Conv2D(out_planes, (3, 3), strides=(1, 1), padding='same')\n",
    "\n",
    "# Upsale the spatial size by a factor of 2\n",
    "def upBlock(out_planes):\n",
    "    block =  tf.keras.Sequential([\n",
    "        tf.keras.layers.UpSampling2D(size=(2, 2)),\n",
    "        conv3x3(out_planes*2),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        GatedLinearUnit2D()\n",
    "    ])\n",
    "    return block\n",
    "\n",
    "# Keep the spatial size\n",
    "def Block3x3_relu(in_planes, out_planes):\n",
    "    block =  tf.keras.Sequential([\n",
    "        conv3x3(out_planes*2),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        GatedLinearUnit2D()\n",
    "\n",
    "    ])\n",
    "    return block\n",
    "\n",
    "class ResBlock(layers.Layer):\n",
    "    def __init__(self, channel_num):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.block =tf.keras.Sequential([\n",
    "            conv3x3(channel_num * 2),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            conv3x3(channel_num),\n",
    "            tf.keras.layers.BatchNormalization()])\n",
    "\n",
    "    def call(self, x):\n",
    "        residual = x\n",
    "        out = self.block(x)\n",
    "        out += residual\n",
    "        return out\n",
    "def Generator():\n",
    "    tam_ini = 256\n",
    "    ca_net = cond_aug()\n",
    "\n",
    "    initializer = tf.random_normal_initializer(0., 0.05)\n",
    "    inputs_noise = tf.keras.layers.Input(shape=(NOISE_SIZE,))#output 100\n",
    "    inputs_emb = tf.keras.layers.Input(shape=(OUT_EMB,))\n",
    "    emb, mu, logvar = ca_net(inputs_emb)\n",
    "    x = tf.concat([inputs_noise, emb], 1)#output 200\n",
    "    x = layers.Dense(4*4*tam_ini*2, use_bias=False)(x) #l x por GLU que me divide el tamaño\n",
    "    x = GatedLinearUnit()(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = layers.Reshape((4,4, tam_ini))(x)\n",
    "\n",
    "    x = upBlock(tam_ini//2)(x)\n",
    "    x = upBlock(tam_ini//4)(x)\n",
    "    x = upBlock(tam_ini//8)(x)  \n",
    "    x = upBlock(tam_ini//16)(x)\n",
    "    #x = upBlock(tam_ini//32)(x)\n",
    "\n",
    "    x = conv3x3(3)(x)\n",
    "    x = tf.keras.activations.tanh(x)\n",
    "    return tf.keras.Model([inputs_noise, inputs_emb], [x, mu, logvar], name=\"generator\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8c356c34",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "generator = Generator()\n",
    "generator.summary()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "91d2f978",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "discriminator = Discriminador()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a237d1cb",
   "metadata": {},
   "source": [
    "### Text encoder"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c92d2229",
   "metadata": {},
   "source": [
    "Aun no claro si conviene o no usar este embedding y entrenarlo junto con el discriminador y generador. Seria una parte de ambas redes."
   ]
  },
  {
   "cell_type": "raw",
   "id": "858e9b0b",
   "metadata": {},
   "source": [
    "num_projection_layers = 2\n",
    "projection_dims = 100\n",
    "dropout_rate = 0.2\n",
    "def get_text_enconder(num_projection_layers=2, projection_dims=100, dropout_rate=0.2):\n",
    "    inputs = layers.Input(shape=(512,), name=\"pesos de clip\")\n",
    "    projected_embeddings = layers.Dense(units=projection_dims)(inputs)\n",
    "    for _ in range(num_projection_layers):\n",
    "        x = tf.nn.gelu(projected_embeddings)\n",
    "        x = layers.Dense(projection_dims)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.Add()([projected_embeddings, x])\n",
    "        projected_embeddings = layers.LayerNormalization()(x)\n",
    "    return tf.keras.Model(inputs, projected_embeddings, name=\"text_encoder\")\n",
    "text_encoder = get_text_enconder(num_projection_layers, projection_dims, dropout_rate)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ffde27bb",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4d8ce9d6",
   "metadata": {},
   "source": [
    "Labels discriminador:\n",
    "+ 0 Imagen falsa para el discriminador\n",
    "+ 0 Imagen verdadera pero mal texto \n",
    "+ 1 Verdadera imagen y texto asociado\n",
    "\n",
    "Labels generador:\n",
    "+ 1 Falsa imagen y generada con texto real"
   ]
  },
  {
   "cell_type": "raw",
   "id": "78733dcf",
   "metadata": {},
   "source": [
    "#def kl_divergence(mu, logvar):\n",
    "#    # Computes KL(p || q). Note that outside of this function, P is the target distribution\n",
    "#    # and Q is the approximating distribution. Inside of this function, P is just the\n",
    "#    # first argument and Q is the second.\n",
    "#    # I inefficiently iterate through all of the q and p values to\n",
    "#    # only compute the log when p_i > 0.\n",
    "#    KLD_element = tf.math.add(tf.math.pow(mu,2),(tf.math.exp(logvar)))\n",
    "#    KLD_element = tf.math.add(tf.math.add(tf.math.multiply(KLD_element,-1),1),logvar)\n",
    "#    KLD = tf.math.multiply(tf.math.reduce_mean(KLD_element),-0.5)\n",
    "#    return KLD\n",
    "\n",
    "def kl_divergence(mean, variance):\n",
    "    return -0.5 * tf.reduce_sum(1 + variance - tf.square(mean) - tf.exp(variance))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c6acaf7a",
   "metadata": {},
   "source": [
    "epochs = 300\n",
    "batch_size = 32\n",
    "learning_rate = 2e-4\n",
    "beta_1 = 0.5\n",
    "num_train_gen = 8 #por cada batch cuantas veces entrenamos el generador\n",
    "cuantas_imgs_vis = 3 #imagenes que vamos viendo al entrenar al final de cada epoca\n",
    "kl_divergence_loss_coeff = 1\n",
    "\n",
    "dataset = tf.data.Dataset.range(len(filenames)) \n",
    "dataset = dataset.shuffle(buffer_size=len(filenames)) # comment this line if you don't want to shuffle data\n",
    "dataset = dataset.batch(batch_size=batch_size, drop_remainder=True)\n",
    "dataset = dataset.repeat(epochs)\n",
    "prepare_caption = PREPARE_CAPTIONS(data_dir, split)\n",
    "filenames, captions = prepare_caption.load_text_data(data_dir, split)\n",
    "prepare_image = PREPARE_IMAGE(data_dir, split, captions, filenames, BASE_SIZE)\n",
    "\n",
    "discriminator = Discriminador()\n",
    "generator = Generator()\n",
    "model_clip = TFCLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "d_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1)\n",
    "g_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1)\n",
    "d_loss_fn = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.1)\n",
    "g_loss_fn = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.1)\n",
    "#kl = tf.keras.losses.KLDivergence()\n",
    "\n",
    "epoca = 0\n",
    "num_batch = 0\n",
    "when_end_epoch =int(len(filenames)/batch_size+1)\n",
    "print(\"EPOCA:\", epoca)\n",
    "\n",
    "disc_loss_tracker_batch = []\n",
    "gen_loss_tracker_batch = []\n",
    "kl_loss_tracker_batch = []\n",
    "disc_loss_tracker_epoch = []\n",
    "gen_loss_tracker_epoch = []\n",
    "kl_loss_tracker_epoch = []\n",
    "\n",
    "for batch in tqdm(dataset):\n",
    "    if num_batch!=0 and num_batch%when_end_epoch==0:\n",
    "        # random_latent_vectors = tf.random.uniform(shape=(cuantas_imgs_vis, NOISE_SIZE), minval=-1, maxval=1)\n",
    "        random_latent_vectors = tf.random.normal(shape=(cuantas_imgs_vis, NOISE_SIZE), mean=0.0, stddev=1.0, dtype=tf.dtypes.float32)\n",
    "        fake_images,  mu, logvar = generator([random_latent_vectors, caps_list_emb[:cuantas_imgs_vis]])\n",
    "        for title, img in zip(caps_list[:cuantas_imgs_vis], fake_images):\n",
    "            img = (img+1)/2\n",
    "            plt.figure(figsize=(3,3))\n",
    "            plt.imshow(img)\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(title)\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "        disc_loss_tracker_epoch.append(np.mean([disc_loss_tracker_batch]))\n",
    "        gen_loss_tracker_epoch.append(np.mean([gen_loss_tracker_batch]))\n",
    "        kl_loss_tracker_epoch.append(np.mean([kl_loss_tracker_batch]))\n",
    "\n",
    "        epoca+=1\n",
    "        print(\"EPOCA:\", epoca)\n",
    "        disc_loss_tracker_batch = []\n",
    "        gen_loss_tracker_batch = []\n",
    "        kl_loss_tracker_batch = []\n",
    "\n",
    "    # Cargo el batch y lo preparo ordenandolo en funcion de mas o menos palabras en el cap\n",
    "    # imgs_list, caps_list, cap_len_list, cls_id_list, key_list, wrong_caps_list, wrong_cap_len_list, wrong_cls_id_list = [], [], [], [], [], [], [], [] \n",
    "    imgs_list, caps_list, cls_id_list, key_list, wrong_caps_list, wrong_cls_id_list = [], [], [], [], [], []\n",
    "\n",
    "    for ind in batch:\n",
    "        imgs, caps, cls_id, key, wrong_caps, wrong_cls_id = prepare_image.__getitem__(ind)\n",
    "        imgs_list.append(imgs)\n",
    "        caps_list.append(caps)\n",
    "        #cap_len_list.append(cap_len)\n",
    "        cls_id_list.append(cls_id)\n",
    "        key_list.append(key)\n",
    "        wrong_caps_list.append(wrong_caps)\n",
    "        #wrong_cap_len_list.append(wrong_cap_len)\n",
    "        wrong_cls_id_list.append(wrong_cls_id)\n",
    "    # ordeno las listas de mas a menos palabras\n",
    "    #sorted_cap_indices = tf.argsort(cap_len_list,direction='DESCENDING').numpy()\n",
    "    #imgs_list = np.array(imgs_list)[sorted_cap_indices]\n",
    "    #caps_list = np.array(caps_list)[sorted_cap_indices]\n",
    "    #cls_id_list = np.array(cls_id_list)[sorted_cap_indices]\n",
    "    #key_list = [key_list[i] for i in sorted_cap_indices]\n",
    "    #sorted_cap_indices = tf.argsort(wrong_cap_len_list,direction='DESCENDING').numpy()\n",
    "    #wrong_caps_list = np.array(wrong_caps_list)[sorted_cap_indices]\n",
    "    #wrong_cap_len_list = np.array(wrong_cap_len_list)[sorted_cap_indices]\n",
    "    #wrong_cls_id_list = np.array(wrong_cls_id_list)[sorted_cap_indices]\n",
    "    \n",
    "    caps_list_token = tokenizer(caps_list, padding='max_length', truncation = True, max_length = WORDS_NUM, return_tensors=\"tf\")\n",
    "    wrong_caps_list_token = tokenizer(wrong_caps_list, padding='max_length', truncation = True, max_length = WORDS_NUM, return_tensors=\"tf\")\n",
    "    caps_list_emb = model_clip(caps_list_token).pooler_output\n",
    "    wrong_caps_list_emb = model_clip(wrong_caps_list_token).pooler_output\n",
    "    \n",
    "    # caps_list_emb = tf.math.divide(tf.subtract(caps_list_emb, tf.reduce_min(caps_list_emb)), tf.subtract(tf.reduce_max(caps_list_emb), tf.reduce_min(caps_list_emb)))\n",
    "    # wrong_caps_list_emb = tf.math.divide(tf.subtract(wrong_caps_list_emb, tf.reduce_min(wrong_caps_list_emb)), tf.subtract(tf.reduce_max(wrong_caps_list_emb), tf.reduce_min(wrong_caps_list_emb)))\n",
    "    # random_latent_vectors = tf.random.uniform(shape=(batch_size, NOISE_SIZE), minval=-1, maxval=1)\n",
    "    random_latent_vectors = tf.random.normal(shape=(batch_size, NOISE_SIZE), mean=0.0, stddev=1.0, dtype=tf.dtypes.float32)\n",
    "\n",
    "    \n",
    "    # TRAIN DISCRIMINATOR\n",
    "    \n",
    "    #indices = tf.range(start=0, limit=batch_size*3, dtype=tf.int32) #por 3\n",
    "    #shuffled_indices = tf.random.shuffle(indices)\n",
    "    labels = tf.concat([tf.zeros((batch_size, 1)), tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0)\n",
    "    #labels = tf.gather(labels, shuffled_indices)\n",
    "    loss_disc = 0\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        embedings = tf.concat([caps_list_emb, caps_list_emb, wrong_caps_list_emb], axis=0)\n",
    "        generated_images, mu, logvar = generator([random_latent_vectors, caps_list_emb])\n",
    "        combined_images = tf.concat([generated_images, imgs_list, imgs_list], axis=0)\n",
    "        #embedings = tf.gather(embedings, shuffled_indices)\n",
    "        #combined_images = tf.gather(combined_images, shuffled_indices)\n",
    "        for i in range(3):\n",
    "            predictions = discriminator([combined_images[i*batch_size:(i+1)*batch_size], embedings[i*batch_size:(i+1)*batch_size]])\n",
    "            d_loss = d_loss_fn(labels[i*batch_size:(i+1)*batch_size], predictions)\n",
    "            loss_disc +=d_loss\n",
    "            grads = tape.gradient(d_loss, discriminator.trainable_weights)            \n",
    "            d_optimizer.apply_gradients((grad, var) for (grad, var) in zip(grads, discriminator.trainable_variables) if grad is not None)\n",
    "            \n",
    "    del tape #Por ser persistente\n",
    "\n",
    "    #del tape #Por ser persistente\n",
    "    disc_loss_tracker_batch.append(loss_disc/3)\n",
    "\n",
    "    misleading_labels = tf.ones((batch_size, 1))    \n",
    "    loss_gen = 0\n",
    "    loss_kl = 0\n",
    "    for _ in range(num_train_gen):\n",
    "        # random_latent_vectors = tf.random.uniform(shape=(batch_size, NOISE_SIZE), minval=-1, maxval=1)\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, NOISE_SIZE), mean=0.0, stddev=1.0, dtype=tf.dtypes.float32)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            fake_images, mu, logvar = generator([random_latent_vectors, caps_list_emb])\n",
    "            predictions = discriminator([fake_images, caps_list_emb])\n",
    "            g_loss = g_loss_fn(misleading_labels, predictions) # añadir termino de divergencia KL\n",
    "            kl_loss = kl_divergence_loss_coeff*kl_divergence(mu, logvar) #kl(misleading_labels, predictions).numpy()\n",
    "            g_loss += kl_loss\n",
    "        loss_kl += kl_loss\n",
    "        loss_gen += g_loss \n",
    "        grads = tape.gradient(g_loss, generator.trainable_weights)\n",
    "        g_optimizer.apply_gradients(zip(grads, generator.trainable_weights))\n",
    "    kl_loss_tracker_batch.append(loss_kl/num_train_gen)\n",
    "    gen_loss_tracker_batch.append(loss_gen/num_train_gen)\n",
    "    if num_batch%(when_end_epoch//10)==0:\n",
    "        print(\"Loss DISC:\", np.mean([disc_loss_tracker_batch]), \"Loss GEN:\", np.mean([gen_loss_tracker_batch]), \"Loss KL:\",np.mean(kl_loss_tracker_batch))\n",
    "        \n",
    "    num_batch += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d5a3e0",
   "metadata": {},
   "source": [
    "# StackGans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ec078e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(out_planes, kernel_size = 3, strides = 1):\n",
    "    \"3x3 convolution with padding\"\n",
    "    return layers.Conv2D(out_planes, (kernel_size, kernel_size), strides=(strides, strides), padding='same', kernel_initializer='glorot_uniform')\n",
    "\n",
    "def dense(units):\n",
    "    return layers.Dense(units, use_bias=False)\n",
    "\n",
    "def batch(momentum = 0.9):\n",
    "    return layers.BatchNormalization(momentum=momentum)\n",
    "\n",
    "def relu():\n",
    "    return layers.Activation(\"relu\")\n",
    "\n",
    "def leaky_relu(alpha = 0.2):\n",
    "    return layers.LeakyReLU(alpha=alpha)\n",
    "\n",
    "def upBlock(out_planes):\n",
    "    \"\"\"\n",
    "    Upsale the spatial size by a factor of 2\n",
    "    \"\"\"\n",
    "    block =  tf.keras.Sequential([\n",
    "        layers.UpSampling2D(size=(2, 2)),\n",
    "        conv3x3(out_planes, 3, 1),\n",
    "        #layers.Dropout(0.2),\n",
    "        batch(),\n",
    "        relu(),\n",
    "    ])\n",
    "    return block\n",
    "\n",
    "def residual_block(x2):\n",
    "    \"\"\"\n",
    "    Residual block in the generator network\n",
    "    \"\"\"\n",
    "    tam_ini = 512\n",
    "\n",
    "    x = conv3x3(tam_ini//2)(x2)\n",
    "    x = batch()(x)\n",
    "    x = relu()(x)\n",
    "    \n",
    "    x = conv3x3(tam_ini//2)(x)\n",
    "    x = batch()(x)\n",
    "    \n",
    "    x = layers.add([x, x2])\n",
    "    x = relu()(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4374222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_stage1_generator():\n",
    "    \"\"\"\n",
    "    Builds a generator model used in Stage-I\n",
    "    \"\"\"\n",
    "    tam_ini = 512\n",
    "\n",
    "    input_layer = keras.Input(shape=(NOISE_SIZE,))\n",
    "\n",
    "    x = dense(tam_ini*4*4)(input_layer)\n",
    "    x = relu()(x)\n",
    "    x = layers.Reshape((4, 4, tam_ini), input_shape=(tam_ini * 4 * 4,))(x)\n",
    "    for i in range(1,5):\n",
    "        x = upBlock(tam_ini//2**i)(x)\n",
    "    x = conv3x3(3)(x)\n",
    "    x = layers.Activation(activation='tanh')(x)\n",
    "    stage1_gen = keras.Model(inputs=input_layer, outputs=x)\n",
    "    return stage1_gen\n",
    "    \n",
    "def build_stage2_generator():\n",
    "    \"\"\"\n",
    "    Create Stage-II generator containing the CA Augmentation Network,\n",
    "    the image encoder and the generator network\n",
    "    \"\"\"\n",
    "    tam_ini = 512\n",
    "    \n",
    "    input_lr_images = keras.Input(shape=(BASE_SIZE, BASE_SIZE, 3))\n",
    "\n",
    "    # 2. Image Encoder\n",
    "    #x = layers.ZeroPadding2D(padding=(1, 1))(input_lr_images)\n",
    "    x = conv3x3(tam_ini//16)(input_lr_images)\n",
    "    x = relu()(x)\n",
    "    \n",
    "    # x = layers.ZeroPadding2D(padding=(1, 1))(x)\n",
    "    x = conv3x3(tam_ini//8, 4, 2)(x)\n",
    "    x = batch()(x)\n",
    "    x = relu()(x)\n",
    "    \n",
    "    #x = layers.ZeroPadding2D(padding=(1, 1))(x)\n",
    "    x = conv3x3(tam_ini//4, 4, 2)(x)\n",
    "    x = batch()(x)\n",
    "    x = relu()(x)\n",
    "    \n",
    "    x = conv3x3(tam_ini//2, 4, 2)(x)\n",
    "    x = batch()(x)\n",
    "    x = relu()(x)\n",
    "    \n",
    "    # 4.layers. Residual blocks\n",
    "    for _ in range(2): #2 para 128, 4 para 256 etc\n",
    "        x = residual_block(x)\n",
    "    \n",
    "    # 5.layers. Upsampling blocks    \n",
    "    for i in range(2,6):\n",
    "        x = upBlock(tam_ini//(2**i))(x)\n",
    "    \n",
    "    x = conv3x3(3)(x)\n",
    "    x = layers.Activation('tanh')(x)\n",
    "\n",
    "    model = keras.Model(inputs=input_lr_images, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "370606fd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_13 (InputLayer)       [(None, 128)]             0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 8192)              1048576   \n",
      "                                                                 \n",
      " activation_37 (Activation)  (None, 8192)              0         \n",
      "                                                                 \n",
      " reshape_2 (Reshape)         (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " sequential_12 (Sequential)  (None, 8, 8, 256)         1180928   \n",
      "                                                                 \n",
      " sequential_13 (Sequential)  (None, 16, 16, 128)       295552    \n",
      "                                                                 \n",
      " sequential_14 (Sequential)  (None, 32, 32, 64)        74048     \n",
      "                                                                 \n",
      " sequential_15 (Sequential)  (None, 64, 64, 32)        18592     \n",
      "                                                                 \n",
      " conv2d_86 (Conv2D)          (None, 64, 64, 3)         867       \n",
      "                                                                 \n",
      " activation_42 (Activation)  (None, 64, 64, 3)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,618,563\n",
      "Trainable params: 2,617,603\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_14 (InputLayer)          [(None, 64, 64, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_87 (Conv2D)             (None, 64, 64, 32)   896         ['input_14[0][0]']               \n",
      "                                                                                                  \n",
      " activation_43 (Activation)     (None, 64, 64, 32)   0           ['conv2d_87[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_88 (Conv2D)             (None, 32, 32, 64)   32832       ['activation_43[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_69 (BatchN  (None, 32, 32, 64)  256         ['conv2d_88[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_44 (Activation)     (None, 32, 32, 64)   0           ['batch_normalization_69[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_89 (Conv2D)             (None, 16, 16, 128)  131200      ['activation_44[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_70 (BatchN  (None, 16, 16, 128)  512        ['conv2d_89[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_45 (Activation)     (None, 16, 16, 128)  0           ['batch_normalization_70[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_90 (Conv2D)             (None, 8, 8, 256)    524544      ['activation_45[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_71 (BatchN  (None, 8, 8, 256)   1024        ['conv2d_90[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_46 (Activation)     (None, 8, 8, 256)    0           ['batch_normalization_71[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_91 (Conv2D)             (None, 8, 8, 256)    590080      ['activation_46[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_72 (BatchN  (None, 8, 8, 256)   1024        ['conv2d_91[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_47 (Activation)     (None, 8, 8, 256)    0           ['batch_normalization_72[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_92 (Conv2D)             (None, 8, 8, 256)    590080      ['activation_47[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_73 (BatchN  (None, 8, 8, 256)   1024        ['conv2d_92[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 8, 8, 256)    0           ['batch_normalization_73[0][0]', \n",
      "                                                                  'activation_46[0][0]']          \n",
      "                                                                                                  \n",
      " activation_48 (Activation)     (None, 8, 8, 256)    0           ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_93 (Conv2D)             (None, 8, 8, 256)    590080      ['activation_48[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_74 (BatchN  (None, 8, 8, 256)   1024        ['conv2d_93[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_49 (Activation)     (None, 8, 8, 256)    0           ['batch_normalization_74[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_94 (Conv2D)             (None, 8, 8, 256)    590080      ['activation_49[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_75 (BatchN  (None, 8, 8, 256)   1024        ['conv2d_94[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 8, 8, 256)    0           ['batch_normalization_75[0][0]', \n",
      "                                                                  'activation_48[0][0]']          \n",
      "                                                                                                  \n",
      " activation_50 (Activation)     (None, 8, 8, 256)    0           ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " sequential_16 (Sequential)     (None, 16, 16, 128)  295552      ['activation_50[0][0]']          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " sequential_17 (Sequential)     (None, 32, 32, 64)   74048       ['sequential_16[0][0]']          \n",
      "                                                                                                  \n",
      " sequential_18 (Sequential)     (None, 64, 64, 32)   18592       ['sequential_17[0][0]']          \n",
      "                                                                                                  \n",
      " sequential_19 (Sequential)     (None, 128, 128, 16  4688        ['sequential_18[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_99 (Conv2D)             (None, 128, 128, 3)  435         ['sequential_19[0][0]']          \n",
      "                                                                                                  \n",
      " activation_55 (Activation)     (None, 128, 128, 3)  0           ['conv2d_99[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,448,995\n",
      "Trainable params: 3,445,571\n",
      "Non-trainable params: 3,424\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gen1 = build_stage1_generator()\n",
    "gen1.summary()\n",
    "gen2 = build_stage2_generator()\n",
    "gen2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "986d7319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_stage1_discriminator():\n",
    "    \"\"\"\n",
    "    Create a model which takes two inputs\n",
    "    1. One from above network\n",
    "    2. One from the embedding layer\n",
    "    3. Concatenate along the axis dimension and feed it to the last module which produces final logits\n",
    "    \"\"\"\n",
    "    tam_ini = 32\n",
    "\n",
    "    input_layer = keras.Input(shape=(BASE_SIZE, BASE_SIZE, 3))\n",
    "    x = conv3x3(tam_ini, 4, 2)(input_layer)\n",
    "    x = leaky_relu()(x)    \n",
    "\n",
    "    for i in range(1,4):\n",
    "        x = conv3x3(tam_ini*(2**i), 4, 2)(x)\n",
    "        x = batch()(x)\n",
    "        x = leaky_relu()(x)\n",
    "        \n",
    "    x = conv3x3(1,1,1)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = dense(1)(x)\n",
    "    x = layers.Activation('sigmoid')(x)\n",
    "\n",
    "    stage1_dis = keras.Model(input_layer, x)\n",
    "    return stage1_dis\n",
    "\n",
    "def build_stage2_discriminator():\n",
    "    \"\"\"\n",
    "    Create Stage-II discriminator network\n",
    "    \"\"\"\n",
    "    \n",
    "    tam_ini = 32\n",
    "    input_layer = keras.Input(shape=(BASE_SIZE*2, BASE_SIZE*2, 3))    \n",
    "\n",
    "    x = conv3x3(tam_ini, 4, 2)(input_layer)\n",
    "    x = leaky_relu()(x)\n",
    "    \n",
    "    for i in range(1,5):\n",
    "        x = conv3x3(tam_ini*(2**i), 4, 2)(x)\n",
    "        x = batch()(x)\n",
    "        x = leaky_relu()(x) \n",
    "    \n",
    "    \n",
    "   # x = conv3x3(tam_ini*8, 1, 1)(x)\n",
    "   # x = batch()(x)\n",
    "   # x = leaky_relu()(x) \n",
    "   # \n",
    "   # x = conv3x3(tam_ini*4, 1, 1)(x)\n",
    "   # x = batch()(x)\n",
    "#\n",
    "   # x2 = conv3x3(tam_ini*2, 3, 1)(x)\n",
    "   # x2 = batch()(x2)\n",
    "   # x2 = leaky_relu()(x2) \n",
    "   # \n",
    "   # x2 = conv3x3(tam_ini*4, 1, 1)(x)\n",
    "   # x2 = batch()(x2)\n",
    "   # \n",
    "   # added_x = layers.add([x, x2])\n",
    "   # added_x = leaky_relu()(added_x)\n",
    "\n",
    "    \n",
    "    #x3 = conv3x3(tam_ini*8, 1, 1)(added_x)\n",
    "    #x3 = batch()(x3)\n",
    "    #x3 = leaky_relu()(x3) \n",
    "    \n",
    "    # x3 = layers.Flatten()(x3)\n",
    "    # x3 = dense(1)(x3)\n",
    "    # x3 = layers.Activation('sigmoid')(x3)\n",
    "    \n",
    "    x = conv3x3(1,1,1)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = dense(1)(x)\n",
    "    x3 = layers.Activation('sigmoid')(x)\n",
    "\n",
    "    stage2_dis = keras.Model(input_layer, x3)\n",
    "    return stage2_dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "145fa72b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_15 (InputLayer)       [(None, 64, 64, 3)]       0         \n",
      "                                                                 \n",
      " conv2d_100 (Conv2D)         (None, 32, 32, 32)        1568      \n",
      "                                                                 \n",
      " leaky_re_lu_48 (LeakyReLU)  (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " conv2d_101 (Conv2D)         (None, 16, 16, 64)        32832     \n",
      "                                                                 \n",
      " batch_normalization_80 (Bat  (None, 16, 16, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_49 (LeakyReLU)  (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " conv2d_102 (Conv2D)         (None, 8, 8, 128)         131200    \n",
      "                                                                 \n",
      " batch_normalization_81 (Bat  (None, 8, 8, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_50 (LeakyReLU)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv2d_103 (Conv2D)         (None, 4, 4, 256)         524544    \n",
      "                                                                 \n",
      " batch_normalization_82 (Bat  (None, 4, 4, 256)        1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_51 (LeakyReLU)  (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " conv2d_104 (Conv2D)         (None, 4, 4, 1)           257       \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 16        \n",
      "                                                                 \n",
      " activation_56 (Activation)  (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 692,209\n",
      "Trainable params: 691,313\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "Model: \"model_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_16 (InputLayer)       [(None, 128, 128, 3)]     0         \n",
      "                                                                 \n",
      " conv2d_105 (Conv2D)         (None, 64, 64, 32)        1568      \n",
      "                                                                 \n",
      " leaky_re_lu_52 (LeakyReLU)  (None, 64, 64, 32)        0         \n",
      "                                                                 \n",
      " conv2d_106 (Conv2D)         (None, 32, 32, 64)        32832     \n",
      "                                                                 \n",
      " batch_normalization_83 (Bat  (None, 32, 32, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_53 (LeakyReLU)  (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " conv2d_107 (Conv2D)         (None, 16, 16, 128)       131200    \n",
      "                                                                 \n",
      " batch_normalization_84 (Bat  (None, 16, 16, 128)      512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_54 (LeakyReLU)  (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " conv2d_108 (Conv2D)         (None, 8, 8, 256)         524544    \n",
      "                                                                 \n",
      " batch_normalization_85 (Bat  (None, 8, 8, 256)        1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_55 (LeakyReLU)  (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " conv2d_109 (Conv2D)         (None, 4, 4, 512)         2097664   \n",
      "                                                                 \n",
      " batch_normalization_86 (Bat  (None, 4, 4, 512)        2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_56 (LeakyReLU)  (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " conv2d_110 (Conv2D)         (None, 4, 4, 1)           513       \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 1)                 16        \n",
      "                                                                 \n",
      " activation_57 (Activation)  (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,792,177\n",
      "Trainable params: 2,790,257\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "disc1 = build_stage1_discriminator()\n",
    "disc1.summary()\n",
    "disc2 = build_stage2_discriminator()\n",
    "disc2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c37d9704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"adaptive_augmenter\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " random_translation (RandomT  (None, 64, 64, 3)        0         \n",
      " ranslation)                                                     \n",
      "                                                                 \n",
      " random_rotation (RandomRota  (None, 64, 64, 3)        0         \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " random_zoom (RandomZoom)    (None, 64, 64, 3)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# adaptive discriminator augmentation\n",
    "max_translation = 0.125\n",
    "max_rotation = 0.125\n",
    "max_zoom = 0.25\n",
    "\n",
    "augmenter1 = keras.Sequential(\n",
    "            [\n",
    "                layers.InputLayer(input_shape=(BASE_SIZE, BASE_SIZE, 3)),\n",
    "                # blitting/x-flip:\n",
    "                # blitting/integer translation:\n",
    "                layers.experimental.preprocessing.RandomTranslation(\n",
    "                    height_factor=max_translation,\n",
    "                    width_factor=max_translation,\n",
    "                    interpolation=\"nearest\",\n",
    "                ),\n",
    "                # geometric/rotation:\n",
    "                layers.experimental.preprocessing.RandomRotation(factor=max_rotation),\n",
    "                # geometric/isotropic and anisotropic scaling:\n",
    "                layers.experimental.preprocessing.RandomZoom(\n",
    "                    height_factor=(-max_zoom, 0.0), width_factor=(-max_zoom, 0.0)\n",
    "                ),\n",
    "            ],\n",
    "            name=\"adaptive_augmenter\",)\n",
    "\n",
    "augmenter2 = keras.Sequential(\n",
    "            [\n",
    "                layers.InputLayer(input_shape=(BASE_SIZE*2, BASE_SIZE*2, 3)),\n",
    "                # blitting/x-flip:\n",
    "                # blitting/integer translation:\n",
    "                layers.experimental.preprocessing.RandomTranslation(\n",
    "                    height_factor=max_translation,\n",
    "                    width_factor=max_translation,\n",
    "                    interpolation=\"nearest\",\n",
    "                ),\n",
    "                # geometric/rotation:\n",
    "                layers.experimental.preprocessing.RandomRotation(factor=max_rotation),\n",
    "                # geometric/isotropic and anisotropic scaling:\n",
    "                layers.experimental.preprocessing.RandomZoom(\n",
    "                    height_factor=(-max_zoom, 0.0), width_factor=(-max_zoom, 0.0)\n",
    "                ),\n",
    "            ],\n",
    "            name=\"adaptive_augmenter\",)\n",
    "\n",
    "augmenter1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "18d874ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step1(real_images):\n",
    "    # Sample random points in the latent space\n",
    "    # TRAIN FIRST GAN\n",
    "    #real_images1 = [x[0] for x in real_images]\n",
    "    #real_images2 = [x[1] for x in real_images]\n",
    "    \n",
    "    # LABELS DISC\n",
    "    labels_fake = tf.zeros((batch_size, 1))\n",
    "    labels_fake += 0.2 * tf.random.uniform(labels_fake.shape)\n",
    "    labels_real = tf.ones((batch_size, 1))\n",
    "    labels_real -= 0.2 * tf.random.uniform(labels_real.shape)\n",
    "    # LABELS GEN\n",
    "    misleading_labels = tf.ones((2*batch_size, 1))\n",
    "    \n",
    "    # AUMENTAMOS LAS IMÁGENES\n",
    "    real_images = augmenter1(tf.convert_to_tensor(real_images))\n",
    "    random_latent_vectors = tf.random.normal(shape=(batch_size, NOISE_SIZE))\n",
    "    generated_images = augmenter1(gen1(random_latent_vectors)) \n",
    "    \n",
    "    # TRAIN DISCRIMINATOR\n",
    "    # Imágenes generadas\n",
    "    loss_disc = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = disc1(generated_images)\n",
    "        d_loss = loss_fn(labels_fake, predictions)\n",
    "        loss_disc += d_loss\n",
    "    grads = tape.gradient(d_loss, disc1.trainable_weights)\n",
    "    d_optimizer.apply_gradients((grad, var) for (grad, var) in zip(grads, disc1.trainable_variables) if grad is not None)\n",
    "\n",
    "    # Imágenes reales    \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = disc1(real_images)\n",
    "        d_loss = loss_fn(labels_real, predictions)\n",
    "        loss_disc +=d_loss\n",
    "    grads = tape.gradient(d_loss, disc1.trainable_weights)\n",
    "    d_optimizer.apply_gradients(zip(grads, disc1.trainable_weights))\n",
    "    d_optimizer.apply_gradients((grad, var) for (grad, var) in zip(grads, disc1.trainable_variables) if grad is not None)\n",
    "    loss_disc /= 2\n",
    "    \n",
    "    # TRAIN GENERATOR\n",
    "    \n",
    "    loss_gen = 0\n",
    "    random_latent_vectors = tf.random.normal(shape=(2*batch_size, NOISE_SIZE))\n",
    "    with tf.GradientTape() as tape:\n",
    "        generated_images = augmenter1(gen1(random_latent_vectors))\n",
    "        predictions = disc1(generated_images)\n",
    "        g_loss = loss_fn(misleading_labels, predictions)\n",
    "        #g_loss = generator_loss(predictions)\n",
    "        loss_gen += g_loss \n",
    "    grads = tape.gradient(g_loss, gen1.trainable_weights)\n",
    "    g_optimizer.apply_gradients(zip(grads, gen1.trainable_weights))\n",
    "    g_optimizer.apply_gradients((grad, var) for (grad, var) in zip(grads, gen1.trainable_variables) if grad is not None)\n",
    "    \n",
    "    return loss_disc, loss_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "79389f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step2(real_images):\n",
    "    \n",
    "    # LABELS DISC\n",
    "    labels_fake = tf.zeros((batch_size, 1))\n",
    "    labels_fake += 0.2 * tf.random.uniform(labels_fake.shape)\n",
    "    labels_real = tf.ones((batch_size, 1))\n",
    "    labels_real -= 0.2 * tf.random.uniform(labels_real.shape)\n",
    "    # LABELS GEN\n",
    "    misleading_labels = tf.ones((2*batch_size, 1))\n",
    "    \n",
    "    # AUMENTAMOS LAS IMÁGENES\n",
    "    real_images = augmenter2(tf.convert_to_tensor(real_images))\n",
    "    random_latent_vectors = tf.random.normal(shape=(batch_size, NOISE_SIZE))\n",
    "    generated_images = augmenter1(gen1(random_latent_vectors)) \n",
    "    generated_images = augmenter2(gen2(generated_images)) \n",
    "\n",
    "    # TRAIN DISCRIMINATOR\n",
    "    loss_disc = 0\n",
    "    # Imágenes generadas\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = disc2(generated_images)\n",
    "        #predictions_real = disc1(real_images)\n",
    "        d_loss = loss_fn(labels_fake, predictions)\n",
    "        #gp = gradient_penalty(batch_size, real_images, generated_images)\n",
    "        #d_loss = discriminator_loss(predictions_real, predictions_fake)+10*gp\n",
    "        loss_disc +=d_loss\n",
    "    grads = tape.gradient(d_loss, disc2.trainable_weights)\n",
    "    d_optimizer.apply_gradients((grad, var) for (grad, var) in zip(grads, disc2.trainable_variables) if grad is not None)\n",
    "    \n",
    "    # Imágenes reales    \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = disc2(real_images)\n",
    "        d_loss = loss_fn(labels_real, predictions)\n",
    "        loss_disc +=d_loss\n",
    "    grads = tape.gradient(d_loss, disc2.trainable_weights)\n",
    "    d_optimizer.apply_gradients((grad, var) for (grad, var) in zip(grads, disc2.trainable_variables) if grad is not None)\n",
    "    loss_disc /= 2\n",
    "\n",
    "    # TRAIN GENERATOR\n",
    "    loss_gen = 0\n",
    "    random_latent_vectors = tf.random.normal(shape=(batch_size, NOISE_SIZE))\n",
    "    generated_images = augmenter1(gen1(random_latent_vectors)) \n",
    "    cov1 = tfp.stats.covariance(generated_images, sample_axis=[1,2])\n",
    "    media_rgb1 = tf.reduce_mean(generated_images, axis=[1,2])\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        generated_images = augmenter2(gen2(generated_images))\n",
    "        predictions = disc2(generated_images)\n",
    "        g_loss = loss_fn(misleading_labels, predictions)\n",
    "        #g_loss = generator_loss(predictions)\n",
    "        loss_gen += g_loss \n",
    "        \n",
    "    media_rgb2 = tf.reduce_mean(generated_images, axis=[1,2])\n",
    "    rgb_media_loss = 0\n",
    "    for i in range(len(media_rgb1)):\n",
    "        rgb_media_loss+=tf.sqrt(tf.reduce_sum(tf.square([media_rgb1[i],media_rgb2[i]])))\n",
    "    rgb_media_loss /= len(media_rgb1)       \n",
    "        \n",
    "    cov2 = tfp.stats.covariance(generated_images, sample_axis=[1,2])\n",
    "    covs_loss = 0\n",
    "    for i in range(len(cov1)):\n",
    "        covs_loss += tf.sqrt(tf.reduce_sum(tf.square([cov1[i],cov2[i]])))\n",
    "    covs_loss /= len(cov1)\n",
    "    \n",
    "    loss_color = 50*(rgb_media_loss+ 5*covs_loss)\n",
    "    g_loss += loss_color\n",
    "    loss_gen += loss_color #Experimental results indicate that the color-consistency regularization is very importan\n",
    "    grads = tape.gradient(g_loss, gen2.trainable_weights)\n",
    "    g_optimizer.apply_gradients((grad, var) for (grad, var) in zip(grads, gen2.trainable_variables) if grad is not None)\n",
    "\n",
    "    return loss_disc, loss_gen, loss_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ebd91a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Load filenames from: ../birds/train/filenames.pickle (8855)\n",
      "Load filenames from: ../birds/train/filenames.pickle (8855)\n",
      "filepath ../birds/captions.pickle\n",
      "Load from:  ../birds/captions.pickle\n",
      "Load filenames from: ../birds/test/filenames.pickle (2933)\n",
      "Load filenames from: ../birds/test/filenames.pickle (2933)\n",
      "filepath ../birds/captions.pickle\n",
      "Load from:  ../birds/captions.pickle\n",
      "Total filenames:  11788 001.Black_footed_Albatross/Black_Footed_Albatross_0046_18.jpg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'EPOCA: 0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a449349fbc94182b45cdce82cff9efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Loss DISC1:0.6967893Loss GEN 1:0.7966633'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Loss DISC1:0.6967893Loss GEN 1:0.7966633'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Loss DISC1:0.6967893Loss GEN 1:0.7966633'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Loss DISC1:0.6967893Loss GEN 1:0.7966633'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [64]\u001b[0m, in \u001b[0;36m<cell line: 55>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m     imgs, caps, cls_id, key, wrong_caps, wrong_cls_id \u001b[38;5;241m=\u001b[39m prepare_image\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(ind)\n\u001b[1;32m     92\u001b[0m     imgs_list\u001b[38;5;241m.\u001b[39mappend(imgs)\n\u001b[0;32m---> 94\u001b[0m loss_disc1, loss_gen1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m disc_loss_tracker_batch1\u001b[38;5;241m.\u001b[39mappend(loss_disc1)\n\u001b[1;32m     96\u001b[0m gen_loss_tracker_batch1\u001b[38;5;241m.\u001b[39mappend(loss_gen1) \n",
      "Input \u001b[0;32mIn [52]\u001b[0m, in \u001b[0;36mtrain_step1\u001b[0;34m(real_images)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m#g_loss = generator_loss(predictions)\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     loss_gen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m g_loss \n\u001b[0;32m---> 50\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m g_optimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(grads, gen1\u001b[38;5;241m.\u001b[39mtrainable_weights))\n\u001b[1;32m     52\u001b[0m g_optimizer\u001b[38;5;241m.\u001b[39mapply_gradients((grad, var) \u001b[38;5;28;01mfor\u001b[39;00m (grad, var) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(grads, gen1\u001b[38;5;241m.\u001b[39mtrainable_variables) \u001b[38;5;28;01mif\u001b[39;00m grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/I+D/TFM/venv/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py:1081\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1077\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_gradients \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1078\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x)\n\u001b[1;32m   1079\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mflatten(output_gradients)]\n\u001b[0;32m-> 1081\u001b[0m flat_grad \u001b[38;5;241m=\u001b[39m \u001b[43mimperative_grad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimperative_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_targets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_sources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflat_sources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconnected_gradients\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent:\n\u001b[1;32m   1090\u001b[0m   \u001b[38;5;66;03m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_watched_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape\u001b[38;5;241m.\u001b[39mwatched_variables()\n",
      "File \u001b[0;32m~/I+D/TFM/venv/lib/python3.8/site-packages/tensorflow/python/eager/imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown value for unconnected_gradients: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m unconnected_gradients)\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_TapeGradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/I+D/TFM/venv/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py:156\u001b[0m, in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    154\u001b[0m     gradient_name_scope \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m forward_pass_name_scope \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mname_scope(gradient_name_scope):\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmock_op\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mout_grads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m grad_fn(mock_op, \u001b[38;5;241m*\u001b[39mout_grads)\n",
      "File \u001b[0;32m~/I+D/TFM/venv/lib/python3.8/site-packages/tensorflow/python/ops/nn_grad.py:577\u001b[0m, in \u001b[0;36m_Conv2DGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    568\u001b[0m shape_0, shape_1 \u001b[38;5;241m=\u001b[39m array_ops\u001b[38;5;241m.\u001b[39mshape_n([op\u001b[38;5;241m.\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], op\u001b[38;5;241m.\u001b[39minputs[\u001b[38;5;241m1\u001b[39m]])\n\u001b[1;32m    570\u001b[0m \u001b[38;5;66;03m# We call the gen_nn_ops backprop functions instead of nn_ops backprop\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;66;03m# functions for performance reasons in Eager mode. gen_nn_ops functions take a\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;66;03m# `explicit_paddings` parameter, but nn_ops functions do not. So if we were\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;66;03m# to use the nn_ops functions, we would have to convert `padding` and\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# `explicit_paddings` into a single `padding` parameter, increasing overhead\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;66;03m# in Eager mode.\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 577\u001b[0m     \u001b[43mgen_nn_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d_backprop_input\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshape_0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdilations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdilations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexplicit_paddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplicit_paddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cudnn_on_gpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cudnn_on_gpu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    587\u001b[0m     gen_nn_ops\u001b[38;5;241m.\u001b[39mconv2d_backprop_filter(\n\u001b[1;32m    588\u001b[0m         op\u001b[38;5;241m.\u001b[39minputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    589\u001b[0m         shape_1,\n\u001b[1;32m    590\u001b[0m         grad,\n\u001b[1;32m    591\u001b[0m         dilations\u001b[38;5;241m=\u001b[39mdilations,\n\u001b[1;32m    592\u001b[0m         strides\u001b[38;5;241m=\u001b[39mstrides,\n\u001b[1;32m    593\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m    594\u001b[0m         explicit_paddings\u001b[38;5;241m=\u001b[39mexplicit_paddings,\n\u001b[1;32m    595\u001b[0m         use_cudnn_on_gpu\u001b[38;5;241m=\u001b[39muse_cudnn_on_gpu,\n\u001b[1;32m    596\u001b[0m         data_format\u001b[38;5;241m=\u001b[39mdata_format)\n\u001b[1;32m    597\u001b[0m ]\n",
      "File \u001b[0;32m~/I+D/TFM/venv/lib/python3.8/site-packages/tensorflow/python/ops/gen_nn_ops.py:1240\u001b[0m, in \u001b[0;36mconv2d_backprop_input\u001b[0;34m(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m   1239\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1240\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mConv2DBackpropInput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_backprop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrides\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muse_cudnn_on_gpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cudnn_on_gpu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpadding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexplicit_paddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplicit_paddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdilations\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   1246\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Train first gan\n",
    "epochs = 500\n",
    "batch_size = 64\n",
    "learning_rate = 2e-4\n",
    "beta_1 = 0.5\n",
    "#beta_2 = 0.999\n",
    "\n",
    "if not os.path.exists(\"models\"):\n",
    "    os.mkdir(\"models\")\n",
    "disc1.save('models/disc1.h5')  \n",
    "gen1.save('models/gen1.h5') \n",
    "augmenter1.save('models/augmenter1.h5') \n",
    "\n",
    "split = \"train\"\n",
    "prepare_caption1 = PREPARE_CAPTIONS(data_dir, split)\n",
    "#filenames, captions, ixtoword, wordtoix, n_words = prepare_caption.load_text_data(data_dir, split)\n",
    "filenames1, captions1 = prepare_caption1.load_text_data(data_dir, split)\n",
    "\n",
    "split = \"test\"\n",
    "prepare_caption2 = PREPARE_CAPTIONS(data_dir, split)\n",
    "#filenames, captions, ixtoword, wordtoix, n_words = prepare_caption.load_text_data(data_dir, split)\n",
    "filenames2, captions2 = prepare_caption2.load_text_data(data_dir, split)\n",
    "\n",
    "filenames = filenames1 + filenames2\n",
    "captions = captions1 + captions2\n",
    "BASE_SIZE = 64\n",
    "prepare_image = PREPARE_IMAGE(data_dir, split, captions, filenames, BASE_SIZE)\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.range(len(filenames)) \n",
    "dataset = dataset.shuffle(buffer_size=len(filenames)) # comment this line if you don't want to shuffle data\n",
    "dataset = dataset.batch(batch_size=batch_size, drop_remainder=True)\n",
    "dataset = dataset.repeat(epochs)\n",
    "\n",
    "\n",
    "#discriminator = Discriminador()\n",
    "#generator = Generator()\n",
    "#model_clip = TFCLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "#tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "d_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1)# , beta_2=beta_2)\n",
    "g_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1)# , beta_2=beta_2)\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy()#from_logits=True)\n",
    "\n",
    "epoca = 0\n",
    "num_batch = 0\n",
    "when_end_epoch =int(len(filenames1)/batch_size)\n",
    "dh2 = display(\"EPOCA: 0\",display_id=True)\n",
    "\n",
    "disc_loss_tracker_batch1 = []\n",
    "gen_loss_tracker_batch1 = []\n",
    "disc_loss_tracker_epoch1 = []\n",
    "gen_loss_tracker_epoch1 = []\n",
    "\n",
    "for batch in tqdm(dataset):\n",
    "    if epoca!=0 and epoca%100==0:\n",
    "        learning_rate = learning_rate/2\n",
    "        d_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1)#, beta_2=beta_2)\n",
    "        g_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1)#, beta_2=beta_2)\n",
    "        disc1.save('models/disc1.h5')  \n",
    "        gen1.save('models/gen1.h5') \n",
    "        augmenter1.save('models/augmenter1.h5') \n",
    "    if num_batch!=0 and num_batch%when_end_epoch==0:\n",
    "        #if epoca%2==0:\n",
    "        random_latent_vectors = tf.random.normal(shape=(8, NOISE_SIZE), mean=0.0, stddev=1.0, dtype=tf.dtypes.float32)\n",
    "        fake_images = gen1([random_latent_vectors])\n",
    "        n_row = 2\n",
    "        n_col = 4\n",
    "        _, axs = plt.subplots(n_row, n_col, figsize=(20,10))\n",
    "        axs = axs.flatten()\n",
    "        for img, ax in zip(fake_images, axs):\n",
    "            ax.imshow((img+1)/2)\n",
    "            ax.axis('off')\n",
    "        plt.subplots_adjust(wspace=0, hspace=0)\n",
    "        plt.suptitle(\"GEN1\")\n",
    "        plt.show()\n",
    "\n",
    "        disc_loss_tracker_epoch1.append(np.mean([disc_loss_tracker_batch1]))\n",
    "        gen_loss_tracker_epoch1.append(np.mean([gen_loss_tracker_batch1]))\n",
    "           \n",
    "        epoca+=1\n",
    "        dh2.update(\"EPOCA:\"+str(epoca))\n",
    "        disc_loss_tracker_batch1 = []\n",
    "        gen_loss_tracker_batch1 = []\n",
    "        \n",
    "    # Cargo el batch y lo preparo ordenandolo en funcion de mas o menos palabras en el cap\n",
    "    # imgs_list, caps_list, cap_len_list, cls_id_list, key_list, wrong_caps_list, wrong_cap_len_list, wrong_cls_id_list = [], [], [], [], [], [], [], [] \n",
    "   \n",
    "    imgs_list = []\n",
    "    for ind in batch:\n",
    "        imgs, caps, cls_id, key, wrong_caps, wrong_cls_id = prepare_image.__getitem__(ind)\n",
    "        imgs_list.append(imgs)\n",
    "\n",
    "    loss_disc1, loss_gen1 = train_step1(imgs_list)\n",
    "    disc_loss_tracker_batch1.append(loss_disc1)\n",
    "    gen_loss_tracker_batch1.append(loss_gen1) \n",
    "    \n",
    "    if num_batch == 0:\n",
    "        dh1 = display(\"Loss DISC1:\", np.mean([disc_loss_tracker_batch1]), \" Loss GEN1:\", np.mean([gen_loss_tracker_batch1]), display_id=True)\n",
    "    if num_batch%(when_end_epoch//10)==0:\n",
    "        dh1.update(\"Loss DISC1:\"+str(np.mean([disc_loss_tracker_batch1]))+ \" Loss GEN1:\"+ str(np.mean([gen_loss_tracker_batch1])))\n",
    "    num_batch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831ec4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,8))\n",
    "plt.plot(disc_loss_tracker_epoch1, label = \"discriminador\")\n",
    "plt.plot(gen_loss_tracker_epoch1, label=\"generator\")\n",
    "plt.legend()\n",
    "plt.title(\"Losses discriminador 1 y generador 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f627ab57",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_latent_vectors = tf.random.normal(shape=(8, NOISE_SIZE), mean=0.0, stddev=1.0, dtype=tf.dtypes.float32)\n",
    "fake_images = gen1([random_latent_vectors])\n",
    "n_row = 2\n",
    "n_col = 4\n",
    "_, axs = plt.subplots(n_row, n_col, figsize=(20,10))\n",
    "axs = axs.flatten()\n",
    "for img, ax in zip(fake_images, axs):\n",
    "    ax.imshow((img+1)/2)\n",
    "    ax.axis('off')\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.suptitle(\"GEN1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86cf06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train second gan\n",
    "epochs = 500\n",
    "batch_size = 64\n",
    "learning_rate = 2e-4\n",
    "beta_1 = 0.5\n",
    "#beta_2 = 0.999\n",
    "\n",
    "if not os.path.exists(\"models\"):\n",
    "    os.mkdir(\"models\")\n",
    "disc2.save('models/disc2.h5')  \n",
    "gen2.save('models/gen2.h5') \n",
    "augmenter2.save('models/augmenter2.h5') \n",
    "\n",
    "split = \"train\"\n",
    "prepare_caption1 = PREPARE_CAPTIONS(data_dir, split)\n",
    "#filenames, captions, ixtoword, wordtoix, n_words = prepare_caption.load_text_data(data_dir, split)\n",
    "filenames1, captions1 = prepare_caption1.load_text_data(data_dir, split)\n",
    "\n",
    "split = \"test\"\n",
    "prepare_caption2 = PREPARE_CAPTIONS(data_dir, split)\n",
    "#filenames, captions, ixtoword, wordtoix, n_words = prepare_caption.load_text_data(data_dir, split)\n",
    "filenames2, captions2 = prepare_caption2.load_text_data(data_dir, split)\n",
    "\n",
    "filenames = filenames1 + filenames2\n",
    "captions = captions1 + captions2\n",
    "BASE_SIZE = 128\n",
    "prepare_image = PREPARE_IMAGE(data_dir, split, captions, filenames, BASE_SIZE)\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.range(len(filenames)) \n",
    "dataset = dataset.shuffle(buffer_size=len(filenames)) # comment this line if you don't want to shuffle data\n",
    "dataset = dataset.batch(batch_size=batch_size, drop_remainder=True)\n",
    "dataset = dataset.repeat(epochs)\n",
    "\n",
    "\n",
    "#discriminator = Discriminador()\n",
    "#generator = Generator()\n",
    "#model_clip = TFCLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "#tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "d_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1)# , beta_2=beta_2)\n",
    "g_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1)# , beta_2=beta_2)\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy()#from_logits=True)\n",
    "\n",
    "epoca = 0\n",
    "num_batch = 0\n",
    "when_end_epoch =int(len(filenames)/batch_size)\n",
    "dh2 = display(\"EPOCA:\", epoca)\n",
    "\n",
    "disc_loss_tracker_batch2 = []\n",
    "gen_loss_tracker_batch2 = []\n",
    "color_loss_tracker_batch2 = []\n",
    "\n",
    "disc_loss_tracker_epoch2 = []\n",
    "gen_loss_tracker_epoch2 = []\n",
    "color_loss_tracker_epoch2 = []\n",
    "\n",
    "for batch in tqdm(dataset):\n",
    "    if epoca!=0 and epoca%100==0:\n",
    "        learning_rate = learning_rate/2\n",
    "        d_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2)\n",
    "        g_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2)\n",
    "        disc2.save('models/disc2.h5')  \n",
    "        gen2.save('models/gen2.h5') \n",
    "        augmenter2.save('models/augmenter2.h5') \n",
    "    if num_batch!=0 and num_batch%when_end_epoch==0:\n",
    "        # random_latent_vectors = tf.random.uniform(shape=(cuantas_imgs_vis, NOISE_SIZE), minval=-1, maxval=1)\n",
    "        random_latent_vectors = tf.random.normal(shape=(8, NOISE_SIZE), mean=0.0, stddev=1.0, dtype=tf.dtypes.float32)\n",
    "        fake_images = gen1([random_latent_vectors])\n",
    "        n_row = 2\n",
    "        n_col = 4\n",
    "        _, axs = plt.subplots(n_row, n_col, figsize=(20,10))\n",
    "        axs = axs.flatten()\n",
    "        for img, ax in zip(fake_images, axs):\n",
    "            ax.imshow((img+1)/2)\n",
    "            ax.axis('off')\n",
    "        plt.subplots_adjust(wspace=0, hspace=0)\n",
    "        plt.suptitle(\"GEN1\")\n",
    "        plt.show()\n",
    "            \n",
    "        fake_images = gen2(fake_images)\n",
    "        n_row = 2\n",
    "        n_col = 4\n",
    "        _, axs = plt.subplots(n_row, n_col, figsize=(20,10))\n",
    "        axs = axs.flatten()\n",
    "        for img, ax in zip(fake_images, axs):\n",
    "            ax.imshow((img+1)/2)\n",
    "            ax.axis('off')\n",
    "        plt.subplots_adjust(wspace=0, hspace=0)\n",
    "        plt.suptitle(\"GEN2\")\n",
    "        plt.show()    \n",
    "            \n",
    "        \n",
    "        disc_loss_tracker_epoch2.append(np.mean([disc_loss_tracker_batch2]))\n",
    "        gen_loss_tracker_epoch2.append(np.mean([gen_loss_tracker_batch2]))\n",
    "        color_loss_tracker_epoch2.append(np.mean([color_loss_tracker_batch2]))\n",
    "        \n",
    "        epoca+=1\n",
    "        dh2.update(\"EPOCA:\", epoca)\n",
    "\n",
    "        disc_loss_tracker_batch2 = []\n",
    "        gen_loss_tracker_batch2 = []\n",
    "        color_loss_tracker_batch2 = []\n",
    "    # Cargo el batch y lo preparo ordenandolo en funcion de mas o menos palabras en el cap\n",
    "    # imgs_list, caps_list, cap_len_list, cls_id_list, key_list, wrong_caps_list, wrong_cap_len_list, wrong_cls_id_list = [], [], [], [], [], [], [], [] \n",
    "    # imgs_list, caps_list, cls_id_list, key_list, wrong_caps_list, wrong_cls_id_list = [], [], [], [], [], []\n",
    "    imgs_list = []\n",
    "    for ind in batch:\n",
    "        imgs, caps, cls_id, key, wrong_caps, wrong_cls_id = prepare_image.__getitem__(ind)\n",
    "        imgs_list.append(imgs)\n",
    "        #caps_list.append(caps)\n",
    "\n",
    "    loss_disc2, loss_gen2, loss_color = train_step2(imgs_list)\n",
    "    disc_loss_tracker_batch2.append(loss_disc2)\n",
    "    gen_loss_tracker_batch2.append(loss_gen2)\n",
    "    color_loss_tracker_batch2.append(loss_color)\n",
    "    if num_batch == 0:\n",
    "        dh1 = display(\"Loss DISC2:\", np.mean([disc_loss_tracker_batch2]), \"Loss GEN 2\", np.mean([gen_loss_tracker_batch2]), \"Loss COLOR\", np.mean([color_loss_tracker_batch2]))\n",
    "\n",
    "    if num_batch%(when_end_epoch//10)==0:\n",
    "        dh1.update(\"Loss DISC2:\", np.mean([disc_loss_tracker_batch2]), \"Loss GEN 2\", np.mean([gen_loss_tracker_batch2]), \"Loss COLOR\", np.mean([color_loss_tracker_batch2]))\n",
    "        \n",
    "    num_batch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fd1892",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,8))\n",
    "plt.plot(disc_loss_tracker_epoch2, label = \"discriminador\")\n",
    "plt.plot(gen_loss_tracker_epoch2, label=\"generator\")\n",
    "plt.plot(color_loss_tracker_epoch2, label=\"generator\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Losses discriminador 2, generador 2 y color\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "904db448",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-23 18:23:41.358355: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gen1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m random_latent_vectors \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, NOISE_SIZE), mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, stddev\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdtypes\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m----> 2\u001b[0m fake_images \u001b[38;5;241m=\u001b[39m \u001b[43mgen1\u001b[49m([random_latent_vectors])\n\u001b[1;32m      3\u001b[0m n_row \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      4\u001b[0m n_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gen1' is not defined"
     ]
    }
   ],
   "source": [
    "random_latent_vectors = tf.random.normal(shape=(8, NOISE_SIZE), mean=0.0, stddev=1.0, dtype=tf.dtypes.float32)\n",
    "fake_images = gen1([random_latent_vectors])\n",
    "n_row = 2\n",
    "n_col = 4\n",
    "_, axs = plt.subplots(n_row, n_col, figsize=(20,10))\n",
    "axs = axs.flatten()\n",
    "for img, ax in zip(fake_images, axs):\n",
    "    ax.imshow((img+1)/2)\n",
    "    ax.axis('off')\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.suptitle(\"GEN1\")\n",
    "plt.show()\n",
    "\n",
    "fake_images = gen2(fake_images)\n",
    "n_row = 2\n",
    "n_col = 4\n",
    "_, axs = plt.subplots(n_row, n_col, figsize=(20,10))\n",
    "axs = axs.flatten()\n",
    "for img, ax in zip(fake_images, axs):\n",
    "    ax.imshow((img+1)/2)\n",
    "    ax.axis('off')\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.suptitle(\"GEN2\")\n",
    "plt.show()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
